{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:22: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from pytorch_lightning_lm.data_module import QuotesDataModule\n",
    "from pytorch_lightning_lm.model import RNNModel\n",
    "from pytorch_lightning_lm.metrics import Perplexity\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torchtext.datasets import LanguageModelingDataset\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from tqdm.autonotebook import tqdm\n",
    "en = spacy.load(\"en\")\n",
    "\n",
    "def spacy_tokenizer(x):\n",
    "    return [tok.text for tok in en.tokenizer(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Models(NLMs)\n",
    "\n",
    "Neural Language models are continuous probability models which approaches the same problem that statistical models tackled from a different perpective. And this approach has consistently beaten the benchmarks set by statistical models, both in the LM itself, as well as the downstream tasks. \n",
    "\n",
    ">A fundamental problem that makes statistical language modelling difficult is the curse of dimensionality. If one wants to model the joint distribution of 10 consecutive words in a natural language with a vocabulary of V of size 100000, there are potentially $100000^{10} - 1 = 10^{50}-1$ free parameters.\n",
    "\n",
    ">When modeling continuous variables, we obtain generalization more easily (e.g. with smooth classes of functions like multi-layer neural networks or Gaussian mixture models) because the function to be learned can be expected to have some local smoothness properties.  - [Bengio et.al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "All the Smoothing and Backoff techniques we have seen earlier is to overcome this shortcoming with a few crutches. Bengio et.al. continues to propose two key points of improvements over the Statistical LMs:\n",
    "- Trigram or Fourgram models were the SOTA at that point. But even there, we resort to all kinds of techniques like Kneser-Ney or Witten-Bell Smoothing to leverage the information from preceeding words from the context.\n",
    "- Statistical LMs also do not consider semantic similarity between two words. _\"A cat is walking in a bedroom\"_ is different from _\"A dog is walking in a room\"_ because the LM doesn't know that cat and dog are similar, or room and bedroom are similar.\n",
    "\n",
    "\n",
    "The neural network approach to language modeling can be described using the three following model properties:\n",
    "\n",
    ">- Associate each word in the vocabulary with a distributed word feature vector.\n",
    ">- Express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence.\n",
    ">- Learn simultaneously the word feature vector and the parameters of the probability function. \n",
    "\n",
    "> [Bengio et.al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "Let's pick the same dataset we were working with and train some Neural models. I have preprocessed the QuotesDB, split into Train, Val and Test, and saved into txt file. And learning from the previous models, have slightly refined the cleaning process where we now replace contractions with full versions and insert spaces between punctuations and words. _(Accompanying code can be found in Appendix-quotes_to_txt.ipynb)_\n",
    "\n",
    "We will be sticking with PyTorch and the transformers library from huggingface for thr remainder of exercise. And for making things easier, we are going to be using PyTorch Lightning, which is an abstraction over pure PyTorch, hiding away the boilerplate code we have to write for training a model. And it also takes up the tedious job of making the training work on GPUs, multi-GPUs etc. seamlessly. the only ting it asks in return is that you structure your code in a specific way. And that specific way is pretty much the way we usually write code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Lightning is an attempt at standardizing PyTorch code, abstract away boilerplate code other slightly technical training aspects like distributed training, mixed precision training, multi-GPU training, etc so that researchers can focus on what they do best and accelerate the research cycle. It also acts as a standard for production systems which makes the code less prone to errors and structured.\n",
    "\n",
    "Below is a diagram from a [medium post](https://towardsdatascience.com/supercharge-your-ai-research-with-pytorch-lightning-337948a99eec) by the author of the library. It shows what parts of the whole cycle have been automated by Pytorch Lightning.\n",
    "\n",
    "![](images/lightning.jpeg)\n",
    "\n",
    "The boxes in Blue are the boxes we need to fill in with our code, and the rest of them are taken care by the framework. If you have written PyTorch code before, porting to PyTorch Lightning is really easy. I strongly advise you to check out [this link](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) to get an overview of what it can do.\n",
    "\n",
    "There is one mandatory requirement and an optional and recommended requirement from Pytorch Lightning:\n",
    "1. (Mandatory) We should be defining our model inheriting LightningModule and not nn.Module. And in this class, we add the training, validation and test steps overriding predefined methods.\n",
    "2. (Optional) We should organize all our data processing and loading to a class inheriting DataModule. This has everything related to data(downloading, loading, splitting, tokenization, batching, etc.).\n",
    "\n",
    "So, let's define our DataModule first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A datamodule encapsulates the five steps involved in data processing\n",
    "> in PyTorch:\n",
    "> \n",
    "> Download / tokenize / process.\n",
    "> \n",
    "> Clean and (maybe) save to disk.\n",
    "> \n",
    "> Load inside Dataset.\n",
    "> \n",
    "> Apply transforms (rotate, tokenize, etc…).\n",
    "> \n",
    "> Wrap inside a DataLoader. \n",
    "\n",
    "> To define a DataModule define 5 methods:\n",
    "> \n",
    "> -   prepare_data (how to download(), tokenize, etc…)\n",
    ">     \n",
    "> -   setup (how to split, etc…)\n",
    ">     \n",
    "> -   train_dataloader\n",
    ">     \n",
    "> -   val_dataloader(s)\n",
    ">     \n",
    "> -   test_dataloader(s)\n",
    "\n",
    "_- PyTorch Lightning Docs_\n",
    "\n",
    "_**N.B.**_ - To use DataModules, you should _**pip install pytorch-lightning==0.9.0rc2**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified DataModule is below, but we are going to use the fullblown one after importing. Some of it like the BPTTIterator, you might not get it now. But I'll explain it when we talk about LSTMs. \n",
    "\n",
    "```python\n",
    "class QuotesDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file: str,\n",
    "        valid_file: str = None,\n",
    "        test_file: str = None,\n",
    "        tokenizer=None,\n",
    "        pretrained_vectors=None,\n",
    "        batch_size=32,\n",
    "        bptt=6,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.train_file = train_file\n",
    "        self.valid_file = valid_file\n",
    "        self.test_file = valid_file\n",
    "        self.tokenizer = spacy_tokenizer if tokenizer is None else tokenizer\n",
    "        self.TEXT = data.Field(lower=True, tokenize=self.tokenizer)\n",
    "        self.pretrained_vectors = pretrained_vectors\n",
    "        self.batch_size = batch_size\n",
    "        self.bptt = bptt\n",
    "        self._load_data()\n",
    "        self._build_vocab()\n",
    "        self.vocab = self.TEXT.vocab\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Read file and tokenize\n",
    "        self.train_data = LanguageModelingDataset(self.train_file, self.TEXT)\n",
    "        if self.valid_file:\n",
    "            self.valid_data = LanguageModelingDataset(self.valid_file, self.TEXT)\n",
    "        else:\n",
    "            self.valid_data = None\n",
    "        if self.test_file:\n",
    "            self.test_data = LanguageModelingDataset(self.test_file, self.TEXT)\n",
    "        else:\n",
    "            self.test_data = None\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        self.TEXT.build_vocab(self.train_data, vectors=self.pretrained_vectors)\n",
    "\n",
    "    @classmethod\n",
    "    def _make_iter(cls, dataset, batch_size, bptt_len):\n",
    "        if dataset:\n",
    "            _iter = data.BPTTIterator(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                bptt_len=bptt_len,  # this is where we specify the sequence length\n",
    "                repeat=False,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        else:\n",
    "            _iter = []\n",
    "        return _iter\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # No action here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # No action here\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._make_iter(self.train_data, self.batch_size, self.bptt)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._make_iter(self.valid_data, self.batch_size, self.bptt)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._make_iter(self.test_data, self.batch_size, self.bptt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Neural Language Models did not start with LSTMs, we will just skip ahead to the good part and look at an LSTM. Before that a quick refresher on LSTMs(rather RNNs). To understand the exact difference between vanilla RNNs and LSTMs, you can refer to this [amazing blog by Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "RNNs are Neural Network architectures specifically suited to sequences. They were designed to overcome the lack of memory in traditional Neural Networks. What you see below is a typical RNN , where you have a sequence, $x_1, x_2, ..., x_t$, where t is the number of timesteps. The core idea in an RNN is the use of a hidden state as kind of a memory to remember the elements of the sequence it has already seen.\n",
    "\n",
    "So what an RNN will do is take in the inputs one by one, and spit out two outputs, a prediction and a hidden state. This hidden state and the next element in the timestep is passed to the network again, and it spits out another prediction and a new hidden state. This continues till you reach the end of the sequence.\n",
    "\n",
    "![lstm](images/lstm.png)\n",
    "\n",
    "Now let's see how this translates into the PyTorch implementation. The RNN implementation in PyTorch(`batch_first=False`) takes in a matrix of shape `(# of timesteps, batch size)` and returns an output of the same shape - `(# of timesteps, batch size)`. Intuitively, we may think that it need not return the intermediate outputs, but just the final output because that's the output after considering all the sequence, right? But there are many usecases, including LMs, where it will be beneficial to have access to intermediate outputs as well. The first approach might be fine for something like a Sentiment analysis, where you accumulate the information over an entire sentence and then make a classification, but if you think about use cases like NER tagging, Text Generation, etc. we would use all or multiple outputs.\n",
    "\n",
    "![lstm](images/lstm_pytorch.png)\n",
    "\n",
    "Now let's think about how we formulate the data for training. Even though there are many ways of training a Language Model, let's choose the simplest one - Next Word Prediction. Just like we did with the Statistical Language Models, we give the model a sequence of words and ask it to predict the next word. Only here, we do it in mini-batches. And how do we do that? We can always write our own iterator which offsets the sequence by one and pose it as the target, but why reinvent the wheel. `torchtext` has a BPTTIterator which does just this job. You provide a sequence of words to the iterator and mention the batch size and the mazimum length of context(bptt), it does the dirty job of batching the sequence into text-target pairs.\n",
    "\n",
    "If the sequence of words are, _the quick brown fox jumped over the lazy dog_, we can use the BPTTIterator to make following text - target pairs(bptt=3), and then train our network.\n",
    "\n",
    "![bptt](images/bptt_training.png)\n",
    "\n",
    "![architecture](images/rnn_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Pytorch Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch Lighning, you have to sub-class LightningModule instead of nn.Module. But LightningModule is just like nn.Module, but has more functionalities baked into it.\n",
    "\n",
    "A LightningModule organizes your PyTorch code into 5 sections\n",
    "\n",
    "- Computations (init).\n",
    "\n",
    "- Train loop (training_step)\n",
    "\n",
    "- Validation loop (validation_step)\n",
    "\n",
    "- Test loop (test_step)\n",
    "\n",
    "- Optimizers (configure_optimizers)\n",
    "\n",
    "A few awesome things about Lightning is that:\n",
    "1. It does not abstract your PyTorch code, but rather organizes it.\n",
    "2. The awesome Trainer object takes care of the heavy lifting and boiler plate training code. i.e. no more optimizer.zero_grad() or aggregation of loss per epoch.\n",
    "3. No more .cuda() or .to() calls. Phew!\n",
    "4. You want to do distributed training, multi-GPU training, TPU training? No worries, Lightning has got your back.\n",
    "\n",
    "The most basic Pytorch Lighning Model has a structure something like below\n",
    "``` python\n",
    "class LitModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        #Set all the parameters of your model\n",
    "        #Create the layers of the model\n",
    "        #Just like regular PyTorch\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Forward Pass\n",
    "        #Just like regular Pytorch\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        #call the forward pass and calculate the loss\n",
    "        # PyTorch training code, which was floating around in your training loop\n",
    "       \n",
    "    def configure_optimizers(self):\n",
    "        # return the optimizer\n",
    "```\n",
    "\n",
    "That's it. You've got your Pytorch Lightning module and ow you can do the magical trainer.fit() to run your model. Take a look at the beautiful documentation for PyTorch Lightning [here](https://pytorch-lightning.readthedocs.io/en/latest/).\n",
    "\n",
    "Now let's define our model. The model wa heavily inspired by the LM model in the [official PyTorch github](https://github.com/pytorch/examples/blob/master/word_language_model/main.py)\n",
    "\n",
    "Below is a simple barebones(might not even run), although the model we are going to use has more bells and whistles and will be imported from another file.\n",
    "\n",
    "``` python\n",
    "class RNNModel(pl.LightningModule):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        ntoken,\n",
    "        ninp,\n",
    "        nhid,\n",
    "        nlayers,\n",
    "        batch_size,\n",
    "        lr=1e-3,\n",
    "        dropout=0.5,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        pretrained_vectors=None,\n",
    "    ):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if pretrained_vectors is not None:\n",
    "            assert pretrained_vectors.shape == torch.Size(\n",
    "                [ntoken, ninp]\n",
    "            ), \"When using pretrained embeddings, the embedding vector should have the dimensions (ntoken, ninp)\"\n",
    "            self.encoder.weight.data.copy_(pretrained_vectors)\n",
    "        if rnn_type in [\"LSTM\", \"GRU\"]:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"\"\"An invalid option for `--model` was supplied,\n",
    "                                options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\"\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(self.batch_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        gain = nn.init.calculate_gain(\"relu\")\n",
    "        nn.init.xavier_uniform_(self.encoder.weight, gain)\n",
    "        nn.init.xavier_uniform_(self.decoder.weight, gain)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Applying the Embedding Layer to the input\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        #Passing the embedded matric to RNN\n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        #Passing it through a decoder, in this case a FF Network\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            return (\n",
    "                weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
    "            )\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, batch_size, self.nhid)\n",
    "    \n",
    "    # Need to reset hidden state every training step to make sure gradients \n",
    "    # are not propagated to all the previous histories as well.\n",
    "    def reset_hidden(self, hidden):\n",
    "        if isinstance(hidden, torch.Tensor):\n",
    "            hidden = hidden.detach().to(self.device_type)\n",
    "        else:\n",
    "            hidden = tuple(self.reset_hidden(v) for v in hidden)\n",
    "        return hidden\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        text, targets = batch.text, batch.target\n",
    "        self.hidden = self.reset_hidden(self.hidden)\n",
    "        output = self(text)\n",
    "        loss = self.criterion(output.view(-1, self.ntoken), targets.view(-1))\n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        text, targets = batch.text, batch.target\n",
    "        self.hidden = self.reset_hidden(self.hidden)\n",
    "        output = self(text)\n",
    "        val_loss = self.criterion(output.view(-1, self.ntoken), targets.view(-1))\n",
    "        result = pl.EvalResult(early_stop_on=val_loss)\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "# add PROGRAM level args\n",
    "parser.add_argument('--project-name', type=str, default='neural_lms')\n",
    "parser.add_argument('--experiment-tag', type=str, default='RNN_LM')\n",
    "parser.add_argument('--use-cuda', type=bool, default=True)\n",
    "parser.add_argument('--use-wandb', type=bool, default=True)\n",
    "parser.add_argument('--log-gradients', type=bool, default=False)\n",
    "parser.add_argument('--unk-cutoff', type=int, default=2)\n",
    "\n",
    "# add model specific args\n",
    "# parser = LitModel.add_model_specific_args(parser)\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--bptt', type=int, default=16)\n",
    "parser.add_argument('--rnn-type', type=str, default=\"LSTM\")\n",
    "parser.add_argument('--nhid', type=int, default=64)\n",
    "parser.add_argument('--nlayers', type=int, default=2)\n",
    "parser.add_argument('--pretrained-vector', type=str, default=\"fasttext.simple.300d\")\n",
    "\n",
    "# add all the available trainer options to argparse\n",
    "parser.add_argument('--max_epochs', type=int, default=25)\n",
    "parser.add_argument('--fast_dev_run', type=bool, default=False)\n",
    "# parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda') if (torch.cuda.is_available()&args.use_cuda) else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = QuotesDataModule(\n",
    "    train_file=\"data/quotesdb/funny_quotes.train.txt\",\n",
    "    valid_file=\"data/quotesdb/funny_quotes.val.txt\",\n",
    "    test_file=\"data/quotesdb/funny_quotes.test.txt\",\n",
    "    tokenizer=None,\n",
    "    batch_size=args.batch_size,\n",
    "    bptt=args.bptt,\n",
    "    unk_limit = args.unk_cutoff,\n",
    "    pretrained_vectors=args.pretrained_vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | metric    | Perplexity       | 0     \n",
      "2 | drop      | Dropout          | 0     \n",
      "3 | encoder   | Embedding        | 6 M   \n",
      "4 | rnn       | LSTM             | 126 K \n",
      "5 | decoder   | Linear           | 1 M   \n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BPTTIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d244cf47484a30a32aed588d9393bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Finding best initial lr', style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.008317637711026709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | metric    | Perplexity       | 0     \n",
      "2 | drop      | Dropout          | 0     \n",
      "3 | encoder   | Embedding        | 6 M   \n",
      "4 | rnn       | LSTM             | 126 K \n",
      "5 | decoder   | Linear           | 1 M   \n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BPTTIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7da4c6e88bc40aba1ce4413b6d680a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:22: RuntimeWarning: The metric you returned None must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:22: RuntimeWarning: Can save best model only with loss available, skipping.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = dm.vocab\n",
    "weight_matrix = vocab.vectors\n",
    "ntoken, ninp = weight_matrix.shape\n",
    "\n",
    "pad_idx = vocab.stoi[\"<pad>\"]\n",
    "\n",
    "ppl = Perplexity(pad_idx)\n",
    "model = RNNModel(\n",
    "    rnn_type=args.rnn_type, ntoken=ntoken, ninp=ninp, nhid=args.nhid, nlayers=args.nlayers, batch_size=args.batch_size, device_type= device.type, pretrained_vectors=weight_matrix, metric=ppl\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(gpus=1 if device.type =='cuda' else 0, max_epochs=args.max_epochs, auto_lr_find=True)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "trainer.save_checkpoint(f\"models/LSTM_LM_unk_2.ckpt\")\n",
    "torch.save(dm.vocab, \"models/LSTM_LM_vocab_unk_2.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start tensorboard.\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "seeds = [\n",
    "    \"when life hands you a lemon,\",\n",
    "    \"life is a\",\n",
    "    \"i'd rather be pissed\",\n",
    "    \"all women may not be beautiful but\",\n",
    "    \"i really need a day between\",\n",
    "    \"it's never too late to\"\n",
    "]\n",
    "\n",
    "def generate_sentences(model, vocab, tokenizer, sampler_func, seeds, sampler_kwargs={}, num_words = 20, device='cpu'):\n",
    "    for seed in seeds:\n",
    "        if isinstance(sampler_func, BeamSearch):\n",
    "            gen_text = \" \".join(sampler_func.generate(text_seed=seed, num_words=20))\n",
    "        else:\n",
    "            gen_text = generate_sentence(model, vocab, tokenizer=tokenizer, seed=seed, sampler=sampler_func,sampler_kwargs=sampler_kwargs, num_words=num_words, device=device)\n",
    "        gen_text = gen_text.replace(\"<unk>\",\"UNK\")\n",
    "        display (Markdown(f\"**{seed}** {gen_text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "trainer = pl.Trainer(gpus=1 if device.type =='cuda' else 0, auto_lr_find=False)\n",
    "model = RNNModel.load_from_checkpoint(\"models/LSTM_LM.ckpt\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "model.hidden = model.init_hidden(1)\n",
    "vocab = torch.load(\"models/LSTM_LM_vocab.sav\")\n",
    "\n",
    "weight_matrix = vocab.vectors\n",
    "ntoken, ninp = weight_matrix.shape\n",
    "assert(model.encoder.weight.data.shape == torch.Size([ntoken,ninp]))\n",
    "bptt = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=spacy_tokenizer)\n",
    "test_data = LanguageModelingDataset(\"data/quotesdb/funny_quotes.test.txt\", TEXT)\n",
    "\n",
    "tokens = test_data.examples[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough, we calculated Perplexity while training the model, to compare to our previous models we need to calculate in the same way. So let's do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def logscore(model, vocab, word, context, device):\n",
    "    inp = torch.LongTensor([vocab.stoi[x] for x in context]).unsqueeze(1).to(device)\n",
    "    word_idx = vocab.stoi[word]\n",
    "    out = F.log_softmax(model(inp), dim=1)[-1,:]\n",
    "    return out[word_idx].item()\n",
    "\n",
    "def perplexity(model, vocab, ngrams, device):\n",
    "    log_score_sum = 0\n",
    "    log_score_count=0\n",
    "    for ngram in tqdm(ngrams):\n",
    "        log_score_sum+=logscore(model, vocab, ngram[-1], ngram[:-1], device) \n",
    "        log_score_count+=1\n",
    "    entropy = -1* (log_score_sum/log_score_count)\n",
    "    return pow(2.0, entropy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning_lm.utils import perplexity\n",
    "\n",
    "from pytorch_lightning_lm.samplers import BeamSearch, DiverseNbestBeamSearch, DiverseBeamSearch, greedy_decoding, weighted_random_choice, topk, nucleus\n",
    "\n",
    "from pytorch_lightning_lm.utils import generate_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306f03bc346f44d48476f6bf7f0a7df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test PPT is 50.50857680006482\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ngrams = nltk.ngrams(tokens,n=bptt+1)\n",
    "test_ppl = perplexity(model, vocab, ngrams, device)\n",
    "print(f\"Test PPT is {test_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** i 'm not a good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i 'm not a good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** be a same . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = greedy_decoding, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** \" he said . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i do n't know . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beam_search=BeamSearch(model=model, vocab=vocab, tokenizer=spacy_tokenizer, beam_width=30,verbose=False, debug_level=0, device = device)\n",
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = beam_search, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diverse N-Best Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** in the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** in the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "div_nbest_bs=DiverseNbestBeamSearch(model=model, vocab=vocab, tokenizer=spacy_tokenizer, beam_width=30,verbose=False, debug_level=0, device = device, diversity_factor=1)\n",
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = div_nbest_bs, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diverse Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** no . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** no . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbs=DiverseBeamSearch(model=model, vocab=vocab, tokenizer=spacy_tokenizer, beam_width=30, num_groups=15, verbose=False, debug_level=0, device = device, diversity_strength=0)\n",
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = dbs, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** the maestro is as you do n't wish to sporting out . i was stolen , but i 'm protecting"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** room in yet ... i want for our pressure . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** , i would make me its business - claim . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i ’m comming , you can mean in . the ' answer is small 12 . it was lying to"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the shore of children , he was starving before ' human exploited and she 'd think i guess , sir"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** get doing my heart <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=weighted_random_choice, sampler_kwargs={\"temperature\":1}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** where when you are just up in his mind , ” he says , “ is what it is hard"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** sex?why is no business . i do n't seem an mind more dumb i am going to rescue a combination"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** on me at the bed that much events , what is much people one of the grail the things are"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** it was n't being ? <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the picture of her ... <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** him . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=weighted_random_choice, sampler_kwargs={\"temperature\":0.9}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** the world and the i would not have already been . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** great . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** to think that i hoped you 're not a good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i think to be a few - the opinion of the beginning of the person , he could actually have"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the one of the time . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** protect you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=weighted_random_choice, sampler_kwargs={\"temperature\":0.5}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** i was not going to be a same thing to be . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** lot of the same . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** to be . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** you 're not a good thing to be a way , and the man , and you are a good"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** a time . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** be the way of the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=topk, sampler_kwargs={\"temperature\":1, \"k\":3}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** the world , and that 's not a only man , but the lot of the great - love ,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** last time . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** to be a same - <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i 'm not a time to be a thing about them . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the word , that was the lot of the same , and i have to have to hold his face"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** be a very same of a same person . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=topk, sampler_kwargs={\"temperature\":0.5, \"k\":50}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-p or Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** \" in your cat with no past would forget his hand . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** pizza who really watched , ” the large hour was the proximity , no , good white and the single"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . what was going to one of his emotions . she said for what i can speak and can go"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** the light and is it to try like never understand the world 's mind ’s style . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** with the house . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** discover you can leave the fifteen in view . the street ? <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=nucleus, sampler_kwargs={\"temperature\":0.9, \"p\":0.9}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** it ’s not a place . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** lot of this ? <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** in the moon . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i was not a man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** your own partner . i am , and you have the little real , and do n't know what i"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** me . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=nucleus, sampler_kwargs={\"temperature\":1, \"p\":0.5}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** and and . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** family . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** they have n't been on the old . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the best and the person . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** go on . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=nucleus, sampler_kwargs={\"temperature\":0.9, \"p\":0.5}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next major innovation in the field of Neural Models was the introduction of Attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning_lm.model import RNNAttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "# add PROGRAM level args\n",
    "parser.add_argument('--project-name', type=str, default='neural_lms')\n",
    "parser.add_argument('--experiment-tag', type=str, default='RNN_LM_w_Att')\n",
    "parser.add_argument('--use-cuda', type=bool, default=True)\n",
    "parser.add_argument('--use-wandb', type=bool, default=False)\n",
    "parser.add_argument('--log-gradients', type=bool, default=False)\n",
    "parser.add_argument('--unk-cutoff', type=int, default=1)\n",
    "\n",
    "# add model specific args\n",
    "# parser = LitModel.add_model_specific_args(parser)\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--bptt', type=int, default=32)\n",
    "parser.add_argument('--rnn-type', type=str, default=\"LSTM\")\n",
    "parser.add_argument('--nhid', type=int, default=128)\n",
    "parser.add_argument('--nlayers', type=int, default=2)\n",
    "parser.add_argument('--att-width', type=int, default=16)\n",
    "parser.add_argument('--pretrained-vector', type=str, default=\"fasttext.simple.300d\")\n",
    "\n",
    "# add all the available trainer options to argparse\n",
    "parser.add_argument('--max_epochs', type=int, default=25)\n",
    "parser.add_argument('--fast_dev_run', type=bool, default=False)\n",
    "# ie: now --gpus --num_nodes ... --fast_dev_run all work in the cli\n",
    "# parser = Trainer.add_argparse_args(parser)\n",
    "args = parser.parse_args()\n",
    "device = torch.device('cuda') if (torch.cuda.is_available()&args.use_cuda) else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "dm = QuotesDataModule(\n",
    "    train_file=\"data/quotesdb/funny_quotes.train.txt\",\n",
    "    valid_file=\"data/quotesdb/funny_quotes.val.txt\",\n",
    "    test_file=\"data/quotesdb/funny_quotes.test.txt\",\n",
    "    tokenizer=None,\n",
    "    batch_size=args.batch_size,\n",
    "    bptt=args.bptt,\n",
    "    unk_limit = args.unk_cutoff,\n",
    "    pretrained_vectors=args.pretrained_vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dm.vocab\n",
    "weight_matrix = vocab.vectors\n",
    "ntoken, ninp = weight_matrix.shape\n",
    "\n",
    "pad_idx = vocab.stoi[\"<pad>\"]\n",
    "\n",
    "ppl = Perplexity(pad_idx)\n",
    "model = RNNAttentionModel(\n",
    "    rnn_type=args.rnn_type, \n",
    "    ntoken=ntoken, \n",
    "    ninp=ninp, \n",
    "    nhid=args.nhid, \n",
    "    attention_width=args.att_width,\n",
    "    nlayers=args.nlayers, \n",
    "    batch_size=args.batch_size, \n",
    "    device_type= device.type, \n",
    "    pretrained_vectors=weight_matrix, metric=ppl\n",
    ")\n",
    "\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "   min_delta=0.01,\n",
    "   patience=5,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=1 if device.type =='cuda' else 0, \n",
    "                     max_epochs=args.max_epochs, \n",
    "                    auto_lr_find=False if args.fast_dev_run else True,\n",
    "                    fast_dev_run=args.fast_dev_run,\n",
    "                    early_stop_callback=early_stop_callback)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "trainer.auto_lr_find = False\n",
    "trainer.save_checkpoint(f\"models/LSTM_w_Att_LM.ckpt\")\n",
    "torch.save(dm.vocab, \"models/LSTM_w_Att_LM_vocab.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "seeds = [\n",
    "    \"when life hands you a lemon,\",\n",
    "    \"life is a\",\n",
    "    \"i'd rather be pissed\",\n",
    "    \"all women may not be beautiful but\",\n",
    "    \"i really need a day between\",\n",
    "    \"it's never too late to\"\n",
    "]\n",
    "\n",
    "def generate_sentences(model, vocab, tokenizer, sampler_func, seeds, sampler_kwargs={}, num_words = 20, device='cpu'):\n",
    "    for seed in seeds:\n",
    "        if isinstance(sampler_func, BeamSearch):\n",
    "            gen_text = \" \".join(sampler_func.generate(text_seed=seed, num_words=20))\n",
    "        else:\n",
    "            gen_text = generate_sentence(model, vocab, tokenizer=tokenizer, seed=seed, sampler=sampler_func,sampler_kwargs=sampler_kwargs, num_words=num_words, device=device)\n",
    "        gen_text = gen_text.replace(\"<unk>\",\"UNK\")\n",
    "        display (Markdown(f\"**{seed}** {gen_text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:22: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ee2f8f7684ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mweight_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mntoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mninp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mntoken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mninp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mbptt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('cpu')\n",
    "trainer = pl.Trainer(gpus=1 if device.type =='cuda' else 0, auto_lr_find=False)\n",
    "model = RNNAttentionModel.load_from_checkpoint(\"models/RNN_LM_w_Att_LSTM_128_32_128_2_16.ckpt\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "model.hidden = model.init_hidden(1)\n",
    "vocab = torch.load(\"models/RNN_LM_w_Att_LSTM_128_32_128_2_16_vocab.sav\")\n",
    "\n",
    "weight_matrix = vocab.vectors\n",
    "ntoken, ninp = weight_matrix.shape\n",
    "assert(model.encoder.weight.data.shape == torch.Size([ntoken,ninp]))\n",
    "bptt = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45947, 300])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22656"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=spacy_tokenizer)\n",
    "test_data = LanguageModelingDataset(\"data/quotesdb/funny_quotes.test.txt\", TEXT)\n",
    "\n",
    "tokens = test_data.examples[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough, we calculated Perplexity while training the model, to compare to our previous models we need to calculate in the same way. So let's do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def logscore(model, vocab, word, context, device):\n",
    "    inp = torch.LongTensor([vocab.stoi[x] for x in context]).unsqueeze(1).to(device)\n",
    "    word_idx = vocab.stoi[word]\n",
    "    out = F.log_softmax(model(inp), dim=1)[-1,:]\n",
    "    return out[word_idx].item()\n",
    "\n",
    "def perplexity(model, vocab, ngrams, device):\n",
    "    log_score_sum = 0\n",
    "    log_score_count=0\n",
    "    for ngram in tqdm(ngrams):\n",
    "        log_score_sum+=logscore(model, vocab, ngram[-1], ngram[:-1], device) \n",
    "        log_score_count+=1\n",
    "    entropy = -1* (log_score_sum/log_score_count)\n",
    "    return pow(2.0, entropy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning_lm.utils import perplexity\n",
    "\n",
    "from pytorch_lightning_lm.samplers import BeamSearch, DiverseNbestBeamSearch, DiverseBeamSearch, greedy_decoding, weighted_random_choice, topk, nucleus\n",
    "\n",
    "from pytorch_lightning_lm.utils import generate_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306f03bc346f44d48476f6bf7f0a7df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test PPT is 50.50857680006482\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ngrams = nltk.ngrams(tokens,n=bptt+1)\n",
    "test_ppl = perplexity(model, vocab, ngrams, device)\n",
    "print(f\"Test PPT is {test_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** i 'm not a good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i 'm not a good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** be a same . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = greedy_decoding, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** \" he said . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i do n't know . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beam_search=BeamSearch(model=model, vocab=vocab, tokenizer=spacy_tokenizer, beam_width=30,verbose=False, debug_level=0, device = device)\n",
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = beam_search, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diverse N-Best Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** in the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** in the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "div_nbest_bs=DiverseNbestBeamSearch(model=model, vocab=vocab, tokenizer=spacy_tokenizer, beam_width=30,verbose=False, debug_level=0, device = device, diversity_factor=1)\n",
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = div_nbest_bs, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diverse Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** no . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** no . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbs=DiverseBeamSearch(model=model, vocab=vocab, tokenizer=spacy_tokenizer, beam_width=30, num_groups=15, verbose=False, debug_level=0, device = device, diversity_strength=0)\n",
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func = dbs, seeds=seeds, sampler_kwargs={}, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** the maestro is as you do n't wish to sporting out . i was stolen , but i 'm protecting"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** room in yet ... i want for our pressure . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** , i would make me its business - claim . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i ’m comming , you can mean in . the ' answer is small 12 . it was lying to"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the shore of children , he was starving before ' human exploited and she 'd think i guess , sir"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** get doing my heart <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=weighted_random_choice, sampler_kwargs={\"temperature\":1}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** where when you are just up in his mind , ” he says , “ is what it is hard"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** sex?why is no business . i do n't seem an mind more dumb i am going to rescue a combination"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** on me at the bed that much events , what is much people one of the grail the things are"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** it was n't being ? <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the picture of her ... <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** him . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=weighted_random_choice, sampler_kwargs={\"temperature\":0.9}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** the world and the i would not have already been . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** great . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** to think that i hoped you 're not a good thing . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i think to be a few - the opinion of the beginning of the person , he could actually have"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the one of the time . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** protect you . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=weighted_random_choice, sampler_kwargs={\"temperature\":0.5}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** i was not going to be a same thing to be . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** lot of the same . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** to be . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** you 're not a good thing to be a way , and the man , and you are a good"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** a time . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** be the way of the world . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=topk, sampler_kwargs={\"temperature\":1, \"k\":3}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** the world , and that 's not a only man , but the lot of the great - love ,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** last time . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** to be a same - <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i 'm not a time to be a thing about them . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the word , that was the lot of the same , and i have to have to hold his face"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** be a very same of a same person . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=topk, sampler_kwargs={\"temperature\":0.5, \"k\":50}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-p or Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** \" in your cat with no past would forget his hand . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** pizza who really watched , ” the large hour was the proximity , no , good white and the single"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . what was going to one of his emotions . she said for what i can speak and can go"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** the light and is it to try like never understand the world 's mind ’s style . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** with the house . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** discover you can leave the fifteen in view . the street ? <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=nucleus, sampler_kwargs={\"temperature\":0.9, \"p\":0.9}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** it ’s not a place . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** lot of this ? <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** in the moon . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** i was not a man . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** your own partner . i am , and you have the little real , and do n't know what i"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** me . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=nucleus, sampler_kwargs={\"temperature\":1, \"p\":0.5}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**when life hands you a lemon,** and and . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**life is a** family . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i'd rather be pissed** . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**all women may not be beautiful but** they have n't been on the old . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**i really need a day between** the best and the person . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**it's never too late to** go on . <eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sentences(model, vocab, tokenizer=spacy_tokenizer, sampler_func=nucleus, sampler_kwargs={\"temperature\":0.9, \"p\":0.5}, seeds=seeds, num_words = 20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
