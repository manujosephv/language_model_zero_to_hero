{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "en = spacy.load('en')\n",
    "from torchtext.datasets import LanguageModelingDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intro about neural LMs__ \n",
    "\n",
    "Let's train the same dataset on some Neural models.\n",
    "\n",
    "starting with simple LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be sticking with PyTorch and the transformers library from huggingface for thr remainder of the workshop. And for making things easier, we are going to be using PyTorch Lightning, which is an abstraction over pure PyTorch, hiding away the boilerplate code we have to write for training a model. And it also takes up the tedious job of making the training work on GPUs, multi-GPUs etc. seamlessly. the only ting it asks in return is that you structure your code in a specific way. And that specific way is pretty much the way we usually write code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning\n",
    "PyTorch Lightning is an attempt at standardizing PyTorch code, abstract away boilerplate code other slightly technical training aspects like distributed training, mixed precision training, multi-GPU training, etc so that researchers can focus on what they do best and accelerate the research cycle. It also acts as a standard for production systems which makes the code less prone to errors and structured.\n",
    "\n",
    "Below is a diagram from a [medium post](https://towardsdatascience.com/supercharge-your-ai-research-with-pytorch-lightning-337948a99eec) by the author of the library. It shows what parts of the whole cycle have been automated by Pytorch Lightning.\n",
    "\n",
    "![](images/lightning.jpeg)\n",
    "\n",
    "The boxes in Blue are the boxes we need to fill in with our code, and the rest of them are taken care by the framework. If you have written PyTorch code before, porting to PyTorch Lightning is really easy. I stringly advise you to check out [this link](https://pytorch-lightning.readthedocs.io/en/latest/new-project.html) to get an overview of what it can do.\n",
    "\n",
    "The primary requirement of the framework is that we define a LightningModule(which is synonymous to nn.Module) which is the model. And in the same module, we define the training steps in specific methods.\n",
    "The data can be packaged into a DataLoader or can have a DataModule wrapping everything that is related to data(downloading, loading, splitting, tokenization, batching, etc.). This is the recommended way as well.\n",
    "\n",
    "So, let's define our DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_N.B._ - I have preprocessed the QuotesDB, split into Train, Val and Test, and saved into txt file. And learning from the previous models, have slightly refined the cleaning process where we now replace contractions with full versions and insert spaces between punctuations and words.\n",
    "\n",
    "> A datamodule encapsulates the five steps involved in data processing\n",
    "> in PyTorch:\n",
    "> \n",
    "> Download / tokenize / process.\n",
    "> \n",
    "> Clean and (maybe) save to disk.\n",
    "> \n",
    "> Load inside Dataset.\n",
    "> \n",
    "> Apply transforms (rotate, tokenize, etc…).\n",
    "> \n",
    "> Wrap inside a DataLoader. \n",
    "\n",
    "_- PyTorch Lightning Docs_\n",
    "\n",
    "> To define a DataModule define 5 methods:\n",
    "> \n",
    "> -   prepare_data (how to download(), tokenize, etc…)\n",
    ">     \n",
    "> -   setup (how to split, etc…)\n",
    ">     \n",
    "> -   train_dataloader\n",
    ">     \n",
    "> -   val_dataloader(s)\n",
    ">     \n",
    "> -   test_dataloader(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning_lm.data_module import QuotesDataModule\n",
    "from pytorch_lightning_lm.model import RNNModel\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'neural_lms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 6\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "rnn_type = 'LSTM'\n",
    "# ninp = 200\n",
    "nhid=32\n",
    "nlayers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "dm = QuotesDataModule(\n",
    "    train_file=\"data/quotesdb/funny_quotes.train.txt\",\n",
    "    valid_file=\"data/quotesdb/funny_quotes.val.txt\",\n",
    "    test_file=\"data/quotesdb/funny_quotes.test.txt\",\n",
    "    tokenizer=None,\n",
    "    batch_size=batch_size,\n",
    "    bptt=bptt,\n",
    "    pretrained_vectors=\"fasttext.simple.300d\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dm.vocab\n",
    "weight_matrix = vocab.vectors\n",
    "ntoken, ninp = weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\n",
    "    rnn_type=rnn_type, ntoken=ntoken, ninp=ninp, nhid=nhid, nlayers=nlayers, batch_size=batch_size, device_type= device.type, pretrained_vectors=weight_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "# wandb_logger = WandbLogger(name='trial_1',project=project)\n",
    "trainer = pl.Trainer(gpus=1 if device.type =='cuda' else 0, max_epochs=5, fast_dev_run=True)#, logger= wandb_logger) #fast_dev_run=True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | drop      | Dropout          | 0     \n",
      "2 | encoder   | Embedding        | 13 M  \n",
      "3 | rnn       | LSTM             | 51 K  \n",
      "4 | decoder   | Linear           | 1 M   \n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BPTTIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f35b124ac81491f94515795bd9d4cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.ones(bptt, dtype=torch.long)\n",
    "toks = dm.TEXT.preprocess(\"When life hands you lemons\")\n",
    "x = dm.TEXT.numericalize([toks]).to('cpu').squeeze(1)\n",
    "length = min(len(x), bptt)\n",
    "seq[-length:] = x[-length:]\n",
    "seq = seq.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.7090, -10.6165, -10.7652,  ..., -10.8997, -10.8639, -10.6925],\n",
       "        [-10.7068, -10.6144, -10.7447,  ..., -10.9029, -10.8717, -10.6778],\n",
       "        [-10.7071, -10.6122, -10.7299,  ..., -10.9087, -10.8767, -10.6698],\n",
       "        [-10.7085, -10.6129, -10.7242,  ..., -10.9129, -10.8805, -10.6679],\n",
       "        [-10.7108, -10.6154, -10.7195,  ..., -10.9155, -10.8800, -10.6670],\n",
       "        [-10.7117, -10.6141, -10.7179,  ..., -10.9189, -10.8809, -10.6644]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden = model.init_hidden(1)\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "out = model(seq)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.TEXT.vocab.itos[torch.argmax(out[-1,:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(self, src):\n",
    "    #src = [sent_len]\n",
    "    src = src.unsqueeze(1)\n",
    "    #src = [sent_len, 1]\n",
    "    generate_step = 0\n",
    "    while generate_step < 20:\n",
    "      out = self.forward(src)\n",
    "      #out = [sent_len + 1, 1, vocab_size]\n",
    "      out = torch.argmax(out[-1, :], dim=1) # [1]\n",
    "      out = out.unsqueeze(0) #[1,1]\n",
    "      src = torch.cat((src, out), dim=0)\n",
    "      generate_step += 1\n",
    "    src = src.squeeze(1)\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_to_sentence(id_tensor, vocab, join=None):\n",
    "    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n",
    "    if isinstance(id_tensor, torch.LongTensor):\n",
    "        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
    "    elif isinstance(id_tensor, np.ndarray):\n",
    "        ids = id_tensor.transpose().reshape(-1)\n",
    "    batch = [vocab.itos[ind] for ind in ids] # denumericalize\n",
    "    if join is None:\n",
    "        return batch\n",
    "    else:\n",
    "        return join.join(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = model(b.text).cpu().data.numpy()\n",
    "word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
