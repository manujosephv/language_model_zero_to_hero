{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:22: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Models(NLMs)\n",
    "\n",
    "Neural Language models are continuous probability models which approaches the same problem that statistical models tackled from a different perpective. And this approach has consistently beaten the benchmarks set by statistical models, both in the LM itself, as well as the downstream tasks. \n",
    "\n",
    ">A fundamental problem that makes statistical language modelling difficult is the curse of dimensionality. If one wants to model the joint distribution of 10 consecutive words in a natural language with a vocabulary of V of size 100000, there are potentially $100000^{10} - 1 = 10^{50}-1$ free parameters.\n",
    "\n",
    ">When modeling continuous variables, we obtain generalization more easily (e.g. with smooth classes of functions like multi-layer neural networks or Gaussian mixture models) because the function to be learned can be expected to have some local smoothness properties.  - [Bengio et.al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "All the Smoothing and Backoff techniques we have seen earlier is to overcome this shortcoming with a few crutches. Bengio et.al. continues to propose two key points of improvements over the Statistical LMs:\n",
    "- Trigram or Fourgram models were the SOTA at that point. But even there, we resort to all kinds of techniques like Kneser-Ney or Witten-Bell Smoothing to leverage the information from preceeding words from the context.\n",
    "- Statistical LMs also do not consider semantic similarity between two words. _\"A cat is walking in a bedroom\"_ is different from _\"A dog is walking in a room\"_ because the LM doesn't know that cat and dog are similar, or room and bedroom are similar.\n",
    "\n",
    "\n",
    "The neural network approach to language modeling can be described using the three following model properties:\n",
    "\n",
    ">- Associate each word in the vocabulary with a distributed word feature vector.\n",
    ">- Express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence.\n",
    ">- Learn simultaneously the word feature vector and the parameters of the probability function. \n",
    "\n",
    "> [Bengio et.al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "Let's pick the same dataset we were working with and train some Neural models. I have preprocessed the QuotesDB, split into Train, Val and Test, and saved into txt file. And learning from the previous models, have slightly refined the cleaning process where we now replace contractions with full versions and insert spaces between punctuations and words. _(Accompanying code can be found in Appendix-quotes_to_txt.ipynb)_\n",
    "\n",
    "We will be sticking with PyTorch and the transformers library from huggingface for thr remainder of exercise. And for making things easier, we are going to be using PyTorch Lightning, which is an abstraction over pure PyTorch, hiding away the boilerplate code we have to write for training a model. And it also takes up the tedious job of making the training work on GPUs, multi-GPUs etc. seamlessly. the only ting it asks in return is that you structure your code in a specific way. And that specific way is pretty much the way we usually write code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning\n",
    "PyTorch Lightning is an attempt at standardizing PyTorch code, abstract away boilerplate code other slightly technical training aspects like distributed training, mixed precision training, multi-GPU training, etc so that researchers can focus on what they do best and accelerate the research cycle. It also acts as a standard for production systems which makes the code less prone to errors and structured.\n",
    "\n",
    "Below is a diagram from a [medium post](https://towardsdatascience.com/supercharge-your-ai-research-with-pytorch-lightning-337948a99eec) by the author of the library. It shows what parts of the whole cycle have been automated by Pytorch Lightning.\n",
    "\n",
    "![](images/lightning.jpeg)\n",
    "\n",
    "The boxes in Blue are the boxes we need to fill in with our code, and the rest of them are taken care by the framework. If you have written PyTorch code before, porting to PyTorch Lightning is really easy. I strongly advise you to check out [this link](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) to get an overview of what it can do.\n",
    "\n",
    "There is one mandatory requirement and an optional and recommended requirement from Pytorch Lightning:\n",
    "1. (Mandatory) We should be defining our model inheriting LightningModule and not nn.Module. And in this class, we add the training, validation and test steps overriding predefined methods.\n",
    "2. (Optional) We should organize all our data processing and loading to a class inheriting DataModule. This has everything related to data(downloading, loading, splitting, tokenization, batching, etc.).\n",
    "\n",
    "So, let's define our DataModule first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A datamodule encapsulates the five steps involved in data processing\n",
    "> in PyTorch:\n",
    "> \n",
    "> Download / tokenize / process.\n",
    "> \n",
    "> Clean and (maybe) save to disk.\n",
    "> \n",
    "> Load inside Dataset.\n",
    "> \n",
    "> Apply transforms (rotate, tokenize, etc…).\n",
    "> \n",
    "> Wrap inside a DataLoader. \n",
    "\n",
    "> To define a DataModule define 5 methods:\n",
    "> \n",
    "> -   prepare_data (how to download(), tokenize, etc…)\n",
    ">     \n",
    "> -   setup (how to split, etc…)\n",
    ">     \n",
    "> -   train_dataloader\n",
    ">     \n",
    "> -   val_dataloader(s)\n",
    ">     \n",
    "> -   test_dataloader(s)\n",
    "\n",
    "_- PyTorch Lightning Docs_\n",
    "\n",
    "_**N.B.**_ - To use DataModules, you should _**pip install pytorch-lightning==0.9.0rc2**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified DataModule is below, but we are going to use the fullblown one after importing. Some of it like the BPTTIterator, you might not get it now. But I'll explain it when we talk about LSTMs. \n",
    "\n",
    "```python\n",
    "class QuotesDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file: str,\n",
    "        valid_file: str = None,\n",
    "        test_file: str = None,\n",
    "        tokenizer=None,\n",
    "        pretrained_vectors=None,\n",
    "        batch_size=32,\n",
    "        bptt=6,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.train_file = train_file\n",
    "        self.valid_file = valid_file\n",
    "        self.test_file = valid_file\n",
    "        self.tokenizer = spacy_tokenizer if tokenizer is None else tokenizer\n",
    "        self.TEXT = data.Field(lower=True, tokenize=self.tokenizer)\n",
    "        self.pretrained_vectors = pretrained_vectors\n",
    "        self.batch_size = batch_size\n",
    "        self.bptt = bptt\n",
    "        self._load_data()\n",
    "        self._build_vocab()\n",
    "        self.vocab = self.TEXT.vocab\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Read file and tokenize\n",
    "        self.train_data = LanguageModelingDataset(self.train_file, self.TEXT)\n",
    "        if self.valid_file:\n",
    "            self.valid_data = LanguageModelingDataset(self.valid_file, self.TEXT)\n",
    "        else:\n",
    "            self.valid_data = None\n",
    "        if self.test_file:\n",
    "            self.test_data = LanguageModelingDataset(self.test_file, self.TEXT)\n",
    "        else:\n",
    "            self.test_data = None\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        self.TEXT.build_vocab(self.train_data, vectors=self.pretrained_vectors)\n",
    "\n",
    "    @classmethod\n",
    "    def _make_iter(cls, dataset, batch_size, bptt_len):\n",
    "        if dataset:\n",
    "            _iter = data.BPTTIterator(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                bptt_len=bptt_len,  # this is where we specify the sequence length\n",
    "                repeat=False,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        else:\n",
    "            _iter = []\n",
    "        return _iter\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # No action here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # No action here\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._make_iter(self.train_data, self.batch_size, self.bptt)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._make_iter(self.valid_data, self.batch_size, self.bptt)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._make_iter(self.test_data, self.batch_size, self.bptt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start the model building.\n",
    "\n",
    "## LSTM\n",
    "\n",
    "Although the Neural Language Models did not start with LSTMs, we will just skip ahead to the good part and look at an LSTM. Before that a quick refresher on LSTMs(rather RNNs). To understand the exact difference between vanilla RNNs and LSTMs, you can refer to this [amazing blog by Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "RNNs are Neural Network architectures specifically suited to sequences. They were designed to overcome the lack of memory in traditional Neural Networks. What you see below is a typical RNN , where you have a sequence, $x_1, x_2, ..., x_t$, where t is the number of timesteps. The core idea in an RNN is the use of a hidden state as kind of a memory to remember the elements of the sequence it has already seen.\n",
    "\n",
    "So what an RNN will do is take in the inputs one by one, and spit out two outputs, a prediction and a hidden state. This hidden state and the next element in the timestep is passed to the network again, and it spits out another prediction and a new hidden state. This continues till you reach the end of the sequence.\n",
    "\n",
    "![lstm](images/lstm.png)\n",
    "\n",
    "Now let's see how this translates into the PyTorch implementation. The RNN implementation in PyTorch(`batch_first=False`) takes in a matrix of shape `(# of timesteps, batch size)` and returns an output of the same shape - `(# of timesteps, batch size)`. Intuitively, we may think that it need not return the intermediate outputs, but just the final output because that's the output after considering all the sequence, right? But there are many usecases, including LMs, where it will be beneficial to have access to intermediate outputs as well. The first approach might be fine for something like a Sentiment analysis, where you accumulate the information over an entire sentence and then make a classification, but if you think about use cases like NER tagging, Text Generation, etc. we would use all or multiple outputs.\n",
    "\n",
    "![lstm](images/lstm_pytorch.png)\n",
    "\n",
    "Now let's think about how we formulate the data for training. Even though there are many ways of training a Language Model, let's choose the simplest one - Next Word Prediction. Just like we did with the Statistical Language Models, we give the model a sequence of words and ask it to predict the next word. Only here, we do it in mini-batches. And how do we do that? We can always write our own iterator which offsets the sequence by one and pose it as the target, but why reinvent the wheel. `torchtext` has a BPTTIterator which does just this job. You provide a sequence of words to the iterator and mention the batch size and the mazimum length of context(bptt), it does the dirty job of batching the sequence into text-target pairs.\n",
    "\n",
    "If the sequence of words are, _the quick brown fox jumped over the lazy dog_, we can use the BPTTIterator to make following text - target pairs(bptt=3):\n",
    "\n",
    "\n",
    "|Text|Target  |\n",
    "|--|--|\n",
    "| the | quick |\n",
    "| quick | brown |\n",
    "| brown | fox |\n",
    "\n",
    "|Text|Target  |\n",
    "|--|--|\n",
    "| fox | jumped |\n",
    "| jumped | over |\n",
    "| over | the |\n",
    "\n",
    "and so on.\n",
    "\n",
    "Now let's get to model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning_lm.data_module import QuotesDataModule\n",
    "from pytorch_lightning_lm.model import RNNModel\n",
    "from pytorch_lightning_lm.metrics import Perplexity\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'neural_lms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 6\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "rnn_type = 'LSTM'\n",
    "# ninp = 200\n",
    "nhid=32\n",
    "nlayers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "dm = QuotesDataModule(\n",
    "    train_file=\"data/quotesdb/funny_quotes.train.txt\",\n",
    "    valid_file=\"data/quotesdb/funny_quotes.val.txt\",\n",
    "    test_file=\"data/quotesdb/funny_quotes.test.txt\",\n",
    "    tokenizer=None,\n",
    "    batch_size=batch_size,\n",
    "    bptt=bptt,\n",
    "    pretrained_vectors=\"fasttext.simple.300d\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dm.vocab\n",
    "weight_matrix = vocab.vectors\n",
    "ntoken, ninp = weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = Perplexity(pad_idx)\n",
    "model = RNNModel(\n",
    "    rnn_type=rnn_type, ntoken=ntoken, ninp=ninp, nhid=nhid, nlayers=nlayers, batch_size=batch_size, device_type= device.type, pretrained_vectors=weight_matrix, metric=ppl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(name='trial_1',project=project)\n",
    "trainer = pl.Trainer(gpus=1 if device.type =='cuda' else 0, max_epochs=2, fast_dev_run=True)#, logger= wandb_logger) #fast_dev_run=True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | metric    | Perplexity       | 0     \n",
      "2 | drop      | Dropout          | 0     \n",
      "3 | encoder   | Embedding        | 13 M  \n",
      "4 | rnn       | LSTM             | 51 K  \n",
      "5 | decoder   | Linear           | 1 M   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445301b2dfc04505974ae4e37ea1dce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"models/example.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = RNNModel.load_from_checkpoint(\"models/example.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887c1732b080450ba63a40c9fca77a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(10.6672, device='cuda:0'),\n",
      " 'test_ppl': tensor(42923.6484, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 10.667179107666016, 'test_ppl': 42923.6484375}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20\n",
    "seed = \"When life hands you lemons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "model.hidden = model.init_hidden(1)\n",
    "model.eval()\n",
    "sentence = []\n",
    "seq = torch.ones(bptt, dtype=torch.long)\n",
    "toks = dm.TEXT.preprocess(seed)\n",
    "x = dm.TEXT.numericalize([toks]).to('cpu').squeeze(1)\n",
    "length = min(len(x), bptt)\n",
    "seq[-length:] = x[-length:]\n",
    "seq = seq.unsqueeze(1)\n",
    "for i in range(num_words):\n",
    "    out = model(seq)\n",
    "    gen_token = torch.argmax(out[-1,:])\n",
    "    sentence.append(gen_token.item())\n",
    "    gen_token = gen_token.unsqueeze(0).unsqueeze(0)\n",
    "    seq = torch.cat([seq, gen_token])[1:]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'his his his his his his his his his his his his his his his his his his his his'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([dm.TEXT.vocab.itos[word] for word in sentence])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
