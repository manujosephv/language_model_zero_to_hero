{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 3. Language Models - Neural Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchtext\n",
    "# from torchtext import data\n",
    "# import spacy\n",
    "# spacy_en = spacy.load('en')\n",
    "\n",
    "# def spacy_tokenizer(text): # create a tokenizer function\n",
    "#     return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# from dataloaders.lm_dataloader import load_quotes_db\n",
    "\n",
    "# train_ds, valid_ds, test_ds = load_quotes_db(\n",
    "#                                     train_path=\"data/quotesdb/funny_quotes.train.txt\",\n",
    "#                                     valid_path=\"data/quotesdb/funny_quotes.val.txt\",\n",
    "#                                     test_path=\"data/quotesdb/funny_quotes.test.txt\",\n",
    "#                                     encoding=\"utf-8\",\n",
    "#                                     tokenizer=spacy_tokenizer,\n",
    "#                                     vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(, batch_size=args.batch_size * args.bptt,\n",
    "#                             shuffle=False, collate_fn=lambda b: collate_batch(b, args, mask_id, cls_id))\n",
    "\n",
    "# train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "#     (train_ds, valid_ds, test_ds),\n",
    "#     batch_size=32,\n",
    "#     bptt_len=30, # this is where we specify the sequence length\n",
    "#     device=0,\n",
    "#     repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "  \n",
    "  def __init__(self, hid_size, vocab_size, n_head, n_layers, pf_size, max_len, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = device\n",
    "    \n",
    "    self.hid_size = hid_size\n",
    "    self.pf_size = pf_size\n",
    "    self.max_len = max_len\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, hid_size)\n",
    "\n",
    "    self.position_enc = nn.Embedding(self.max_len, self.hid_size)\n",
    "    self.position_enc.weight.data = self.position_encoding_init(self.max_len, self.hid_size)\n",
    "    self.scale = torch.sqrt(torch.FloatTensor([self.hid_size])).to(device)\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(self.hid_size)\n",
    "    self.decoder_layer = nn.TransformerDecoderLayer(d_model=hid_size, nhead = n_head, dim_feedforward=self.pf_size)\n",
    "    self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=n_layers, norm=self.layer_norm)\n",
    "    self.fc = nn.Linear(hid_size, vocab_size)\n",
    "\n",
    "    self._init_weights()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    sent_len, batch_size = x.shape[0], x.shape[1]\n",
    "    memory_mask = self.generate_complete_mask(sent_len)\n",
    "    tgt_mask = self.generate_triangular_mask(sent_len)\n",
    "    memory = torch.zeros(1, batch_size, self.hid_size, device=self.device)\n",
    "\n",
    "    temp = x\n",
    "    temp = self.embedding(temp)\n",
    "\n",
    "    pos = torch.arange(0,sent_len).unsqueeze(1).repeat(1,batch_size).to(self.device)\n",
    "    temp_pos_emb = self.position_enc(pos)\n",
    "\n",
    "    temp = temp * self.scale + temp_pos_emb\n",
    "    temp = self.decoder(temp, memory, tgt_mask=tgt_mask)\n",
    "    temp = self.fc(temp)\n",
    "    return temp\n",
    "\n",
    "  def _init_weights(self):\n",
    "    for p in self.parameters():\n",
    "      if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "  def append_decoder_layer(self):\n",
    "    appended_mod = nn.TransformerDecoderLayer(d_model=hid_size, nhead = n_head).to(self.device)\n",
    "    for p in appended_mod.parameters():\n",
    "      if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    model.decoder.layers.append(appended_mod)\n",
    "    model.decoder.num_layers += 1\n",
    "\n",
    "  def generate_triangular_mask(self, size):\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "        return\n",
    "        \n",
    "  def generate_complete_mask(self, size):\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = torch.empty(size, size).to(device)\n",
    "        mask.fill_(float('-inf'))\n",
    "        return mask\n",
    "\n",
    "  def generate_sequence(self, src):\n",
    "    #src = [sent_len]\n",
    "    src = src.unsqueeze(1)\n",
    "    #src = [sent_len, 1]\n",
    "    generate_step = 0\n",
    "    while generate_step < 20:\n",
    "      out = self.forward(src)\n",
    "      #out = [sent_len + 1, 1, vocab_size]\n",
    "      out = torch.argmax(out[-1, :], dim=1) # [1]\n",
    "      out = out.unsqueeze(0) #[1,1]\n",
    "      src = torch.cat((src, out), dim=0)\n",
    "      generate_step += 1\n",
    "    src = src.squeeze(1)\n",
    "    return src\n",
    "  \n",
    "  def position_encoding_init(self, n_position, d_pos_vec):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    temp = torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
    "    temp = temp.to(self.device)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    " \n",
    "my_tok = spacy.load('en')\n",
    " \n",
    "def spacy_tok(x):\n",
    "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
    " \n",
    "TEXT = data.Field(lower=True, tokenize=spacy_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import LanguageModelingDataset\n",
    "train_dataset = LanguageModelingDataset(\"data/quotesdb/funny_quotes.train.txt\", TEXT)\n",
    "val_dataset = LanguageModelingDataset(\"data/quotesdb/funny_quotes.val.txt\", TEXT)\n",
    "test_dataset = LanguageModelingDataset(\"data/quotesdb/funny_quotes.test.txt\", TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46083\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_dataset)\n",
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_iter(batch_size, bptt_len):\n",
    "  train_iter = data.BPTTIterator(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    bptt_len=bptt_len, # this is where we specify the sequence length\n",
    "    device=device,\n",
    "    repeat=False,\n",
    "    shuffle=True)\n",
    "  print(len(train_iter))\n",
    "  return train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "hid_size = 16\n",
    "pf_size = 64\n",
    "max_len = 512\n",
    "n_head = 4\n",
    "n_layer= 1\n",
    "model = LM(hid_size, vocab_size, n_head, n_layer, pf_size, max_len, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,537,763 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model,train_iter, optimizer, criterion, clip):\n",
    "  epoch_loss = 0\n",
    "  model.train()\n",
    "  for batch in train_iter:\n",
    "    optimizer.zero_grad()\n",
    "    batch_text = batch.text\n",
    "    batch_target = batch.target\n",
    "    result = model(batch_text)\n",
    "    loss = criterion(result.view(-1, result.shape[-1]), batch_target.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_iter)\n",
    "    print(\"epoch is {} loss is {}\".format(epoch, epoch_loss / len(train_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, optimizer, criterion, clip, N_EPOCH):\n",
    "  for epoch in range(N_EPOCH):\n",
    "    epoch_loss = train_one_epoch(model, train_iter, optimizer, criterion, clip)\n",
    "    print(\"epoch is {} loss is {}\".format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BPTTIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\manujoseph\\Anaconda3\\envs\\bot\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is 0 loss is 0.003361069362362536\n",
      "epoch is 1 loss is 0.003311016217512278\n",
      "epoch is 2 loss is 0.0032280897788607953\n",
      "epoch is 3 loss is 0.003154097292042405\n",
      "epoch is 4 loss is 0.0030716204128998746\n",
      "epoch is 5 loss is 0.002980860668109633\n",
      "epoch is 6 loss is 0.0028838386905308848\n",
      "epoch is 7 loss is 0.002786394580150627\n",
      "epoch is 8 loss is 0.002680261867722335\n",
      "epoch is 9 loss is 0.002570513897890149\n",
      "epoch is 10 loss is 0.0024583458192425243\n",
      "epoch is 11 loss is 0.002345041171876145\n",
      "epoch is 12 loss is 0.0022309323555605603\n",
      "epoch is 13 loss is 0.0021198197579749044\n",
      "epoch is 14 loss is 0.0020159621207406276\n",
      "epoch is 15 loss is 0.0019205824663282075\n",
      "epoch is 16 loss is 0.0018345511455243734\n",
      "epoch is 17 loss is 0.0017609125228552716\n",
      "epoch is 18 loss is 0.0016973400980504314\n",
      "epoch is 19 loss is 0.0016421805176372712\n",
      "epoch is 20 loss is 0.0015921920640426116\n",
      "epoch is 21 loss is 0.0015471985504827712\n",
      "epoch is 22 loss is 0.0015043487918492443\n",
      "epoch is 23 loss is 0.0014646929031090648\n",
      "epoch is 24 loss is 0.0014254242675533217\n",
      "epoch is 25 loss is 0.001387901848724165\n",
      "epoch is 26 loss is 0.0013523149505262264\n",
      "epoch is 27 loss is 0.0013164841931251558\n",
      "epoch is 28 loss is 0.0012808357338936636\n",
      "epoch is 29 loss is 0.0012462529662103346\n",
      "epoch is 30 loss is 0.0012121567989968256\n",
      "epoch is 31 loss is 0.0011786895082979956\n",
      "epoch is 32 loss is 0.0011449848424274127\n",
      "epoch is 33 loss is 0.001110334774970114\n",
      "epoch is 34 loss is 0.001076777825768421\n",
      "epoch is 35 loss is 0.0010432993557051443\n",
      "epoch is 36 loss is 0.0010085306379265173\n",
      "epoch is 37 loss is 0.0009741900898062016\n",
      "epoch is 38 loss is 0.0009415100499517138\n",
      "epoch is 39 loss is 0.0009058572680326356\n",
      "epoch is 40 loss is 0.000872414646464685\n",
      "epoch is 41 loss is 0.0008396082052628522\n",
      "epoch is 42 loss is 0.0008063489252718287\n",
      "epoch is 43 loss is 0.0007719273640238221\n",
      "epoch is 44 loss is 0.0007388287687644469\n",
      "epoch is 45 loss is 0.0007046410798206669\n",
      "epoch is 46 loss is 0.0006723856322278676\n",
      "epoch is 47 loss is 0.0006407804510003889\n",
      "epoch is 48 loss is 0.0006075370457068798\n",
      "epoch is 49 loss is 0.0005772235282475816\n",
      "epoch is 50 loss is 0.0005467474255646792\n",
      "epoch is 51 loss is 0.0005173919796384696\n",
      "epoch is 52 loss is 0.00048739062729907356\n",
      "epoch is 53 loss is 0.00045876310705654174\n",
      "epoch is 54 loss is 0.00043167464991739746\n",
      "epoch is 55 loss is 0.0004062769971217316\n",
      "epoch is 56 loss is 0.0003787101004488135\n",
      "epoch is 57 loss is 0.00035536665587024264\n",
      "epoch is 58 loss is 0.00033251726467112296\n",
      "epoch is 59 loss is 0.0003110495870208025\n",
      "epoch is 60 loss is 0.0002938009828804209\n",
      "epoch is 61 loss is 0.00026989201151903587\n",
      "epoch is 62 loss is 0.00025415325507628764\n",
      "epoch is 63 loss is 0.00023609260053476642\n",
      "epoch is 64 loss is 0.0002208954470349163\n",
      "epoch is 65 loss is 0.00020414896404866167\n",
      "epoch is 66 loss is 0.0001956569697566985\n",
      "epoch is 67 loss is 0.00018031120523880257\n",
      "epoch is 68 loss is 0.00016728750651014637\n",
      "epoch is 69 loss is 0.00016263290359363515\n",
      "epoch is 70 loss is 0.00014945845113541417\n",
      "epoch is 71 loss is 0.0001387828009021994\n",
      "epoch is 72 loss is 0.00012963434501378452\n",
      "epoch is 73 loss is 0.00012235943881300473\n",
      "epoch is 74 loss is 0.00011630347461467909\n",
      "epoch is 75 loss is 0.00010755439220498822\n",
      "epoch is 76 loss is 0.00010284548552567677\n",
      "epoch is 77 loss is 9.59244714225967e-05\n",
      "epoch is 78 loss is 9.288154158155484e-05\n",
      "epoch is 79 loss is 8.649239505816714e-05\n",
      "epoch is 80 loss is 7.892817622164184e-05\n",
      "epoch is 81 loss is 7.547587668683313e-05\n",
      "epoch is 82 loss is 7.159948330366749e-05\n",
      "epoch is 83 loss is 7.124202778355037e-05\n",
      "epoch is 84 loss is 6.522253160366382e-05\n",
      "epoch is 85 loss is 6.201065361742006e-05\n",
      "epoch is 86 loss is 5.558381866164116e-05\n",
      "epoch is 87 loss is 5.2067792091417326e-05\n",
      "epoch is 88 loss is 5.211147079992458e-05\n",
      "epoch is 89 loss is 4.908491406972873e-05\n",
      "epoch is 90 loss is 4.395148965782506e-05\n",
      "epoch is 91 loss is 4.7280806867582196e-05\n",
      "epoch is 92 loss is 4.037173605218609e-05\n",
      "epoch is 93 loss is 4.313937404372611e-05\n",
      "epoch is 94 loss is 3.867288722437447e-05\n",
      "epoch is 95 loss is 3.668125317073606e-05\n",
      "epoch is 96 loss is 3.376464316083998e-05\n",
      "epoch is 97 loss is 3.263992456839807e-05\n",
      "epoch is 98 loss is 3.0953603615869615e-05\n",
      "epoch is 99 loss is 3.210868349892156e-05\n",
      "epoch is 100 loss is 2.8870571270776042e-05\n",
      "epoch is 101 loss is 2.8747027648467874e-05\n",
      "epoch is 102 loss is 2.873546399336824e-05\n",
      "epoch is 103 loss is 2.4838409747284403e-05\n",
      "epoch is 104 loss is 2.4233557220994403e-05\n",
      "epoch is 105 loss is 2.6567126110442992e-05\n",
      "epoch is 106 loss is 2.3137341349823247e-05\n",
      "epoch is 107 loss is 2.0394978038069977e-05\n",
      "epoch is 108 loss is 2.1457835151538808e-05\n",
      "epoch is 109 loss is 1.988365486958281e-05\n",
      "epoch is 110 loss is 2.0927460482806926e-05\n",
      "epoch is 111 loss is 1.8895355965354064e-05\n",
      "epoch is 112 loss is 1.8236402302244448e-05\n",
      "epoch is 113 loss is 1.8009198023103557e-05\n",
      "epoch is 114 loss is 1.8286278570916884e-05\n",
      "epoch is 115 loss is 1.8810779415245687e-05\n",
      "epoch is 116 loss is 1.686417840353658e-05\n",
      "epoch is 117 loss is 1.60813888243192e-05\n",
      "epoch is 118 loss is 1.5547212007158284e-05\n",
      "epoch is 119 loss is 1.4794576434501822e-05\n",
      "epoch is 120 loss is 1.3885496137066907e-05\n",
      "epoch is 121 loss is 1.4586304874858397e-05\n",
      "epoch is 122 loss is 1.3453916772599145e-05\n",
      "epoch is 123 loss is 1.3874614889005379e-05\n",
      "epoch is 124 loss is 1.2650275349654269e-05\n",
      "epoch is 125 loss is 1.4001627607843435e-05\n",
      "epoch is 126 loss is 1.3458572512084375e-05\n",
      "epoch is 127 loss is 1.4677616992045954e-05\n",
      "epoch is 128 loss is 1.1336690711021125e-05\n",
      "epoch is 129 loss is 1.136284925930945e-05\n",
      "epoch is 130 loss is 1.0970603789937388e-05\n",
      "epoch is 131 loss is 1.2961072161928793e-05\n",
      "epoch is 132 loss is 1.0134395480938501e-05\n",
      "epoch is 133 loss is 1.0697784210302264e-05\n",
      "epoch is 134 loss is 1.0841515901573899e-05\n",
      "epoch is 135 loss is 1.1033011427444382e-05\n",
      "epoch is 136 loss is 1.0216860761490417e-05\n",
      "epoch is 137 loss is 9.530578210376956e-06\n",
      "epoch is 138 loss is 8.368381380885792e-06\n",
      "epoch is 139 loss is 1.1301936861797212e-05\n",
      "epoch is 140 loss is 8.852444367879776e-06\n",
      "epoch is 141 loss is 9.789664639499494e-06\n",
      "epoch is 142 loss is 8.92799241668025e-06\n",
      "epoch is 143 loss is 9.302200097969153e-06\n",
      "epoch is 144 loss is 9.388600281902909e-06\n",
      "epoch is 145 loss is 7.901627776510234e-06\n",
      "epoch is 146 loss is 7.280409126477004e-06\n",
      "epoch is 147 loss is 7.7240121303107e-06\n",
      "epoch is 148 loss is 9.258791557436029e-06\n",
      "epoch is 149 loss is 8.701619020511523e-06\n",
      "epoch is 150 loss is 7.823205699433235e-06\n",
      "epoch is 151 loss is 1.0747435727168634e-05\n",
      "epoch is 152 loss is 9.531595416089899e-06\n",
      "epoch is 153 loss is 9.602630961012117e-06\n",
      "epoch is 154 loss is 8.854433945396164e-06\n",
      "epoch is 155 loss is 6.974033985744606e-06\n",
      "epoch is 156 loss is 7.324417393332014e-06\n",
      "epoch is 157 loss is 6.196372939967185e-06\n",
      "epoch is 158 loss is 7.612613257149973e-06\n",
      "epoch is 159 loss is 6.493409817835136e-06\n",
      "epoch is 160 loss is 6.34494436443057e-06\n",
      "epoch is 161 loss is 6.246853593030025e-06\n",
      "epoch is 162 loss is 6.159661537447956e-06\n",
      "epoch is 163 loss is 5.820699713870338e-06\n",
      "epoch is 164 loss is 6.675953118269725e-06\n",
      "epoch is 165 loss is 5.412177788574049e-06\n",
      "epoch is 166 loss is 5.288221344943343e-06\n",
      "epoch is 167 loss is 6.6232546423330425e-06\n",
      "epoch is 168 loss is 6.471656637849119e-06\n",
      "epoch is 169 loss is 5.932781576327735e-06\n",
      "epoch is 170 loss is 7.444353341571836e-06\n",
      "epoch is 171 loss is 5.989505205723821e-06\n",
      "epoch is 172 loss is 5.367917986018988e-06\n",
      "epoch is 173 loss is 7.664424953292258e-06\n",
      "epoch is 174 loss is 4.954744397047424e-06\n",
      "epoch is 175 loss is 5.816671066859544e-06\n",
      "epoch is 176 loss is 5.503573750994957e-06\n",
      "epoch is 177 loss is 5.4035056295078894e-06\n",
      "epoch is 178 loss is 6.145686167372357e-06\n",
      "epoch is 179 loss is 4.651605158289659e-06\n",
      "epoch is 180 loss is 5.961473531632097e-06\n",
      "epoch is 181 loss is 6.736778759255786e-06\n",
      "epoch is 182 loss is 5.042202544793668e-06\n",
      "epoch is 183 loss is 4.9329353202391725e-06\n",
      "epoch is 184 loss is 4.541670665419746e-06\n",
      "epoch is 185 loss is 4.767292365781141e-06\n",
      "epoch is 186 loss is 5.382005149739054e-06\n",
      "epoch is 187 loss is 4.903229652773257e-06\n",
      "epoch is 188 loss is 4.513697799443081e-06\n",
      "epoch is 189 loss is 4.369183552660172e-06\n",
      "epoch is 190 loss is 4.565985200832918e-06\n",
      "epoch is 191 loss is 4.174383127175111e-06\n",
      "epoch is 192 loss is 4.12142641956488e-06\n",
      "epoch is 193 loss is 4.493055568798962e-06\n",
      "epoch is 194 loss is 4.610263635662057e-06\n",
      "epoch is 195 loss is 4.023242195664662e-06\n",
      "epoch is 196 loss is 3.9941144198244454e-06\n",
      "epoch is 197 loss is 3.8824523657923675e-06\n",
      "epoch is 198 loss is 4.912676215730782e-06\n",
      "epoch is 199 loss is 4.338430402035935e-06\n",
      "epoch is 200 loss is 3.92718874698283e-06\n",
      "epoch is 201 loss is 4.564245412240889e-06\n",
      "epoch is 202 loss is 4.6105669923743895e-06\n",
      "epoch is 203 loss is 4.244651255308744e-06\n",
      "epoch is 204 loss is 3.1932075742297936e-06\n",
      "epoch is 205 loss is 3.524964239195609e-06\n",
      "epoch is 206 loss is 3.3962102761191104e-06\n",
      "epoch is 207 loss is 3.287830995876143e-06\n",
      "epoch is 208 loss is 3.718800444497135e-06\n",
      "epoch is 209 loss is 3.0495539056058598e-06\n",
      "epoch is 210 loss is 3.961414487688345e-06\n",
      "epoch is 211 loss is 2.9212619647006385e-06\n",
      "epoch is 212 loss is 3.856537201584187e-06\n",
      "epoch is 213 loss is 3.144244869245742e-06\n",
      "epoch is 214 loss is 3.669964089621675e-06\n",
      "epoch is 215 loss is 3.3166894771300765e-06\n",
      "epoch is 216 loss is 3.491388881307127e-06\n",
      "epoch is 217 loss is 2.8738582570242038e-06\n",
      "epoch is 218 loss is 2.976919769793609e-06\n",
      "epoch is 219 loss is 2.584618112470031e-06\n",
      "epoch is 220 loss is 2.8608427401929023e-06\n",
      "epoch is 221 loss is 3.1807105586505673e-06\n",
      "epoch is 222 loss is 3.0559144981692233e-06\n",
      "epoch is 223 loss is 2.6242809836549214e-06\n",
      "epoch is 224 loss is 3.045487120659063e-06\n",
      "epoch is 225 loss is 2.4587358486357985e-06\n",
      "epoch is 226 loss is 3.1193728212566443e-06\n",
      "epoch is 227 loss is 2.787127059096282e-06\n",
      "epoch is 228 loss is 2.395099499992148e-06\n",
      "epoch is 229 loss is 3.826844052677417e-06\n",
      "epoch is 230 loss is 2.4365890618598847e-06\n",
      "epoch is 231 loss is 3.5607810014298128e-06\n",
      "epoch is 232 loss is 2.626842921340641e-06\n",
      "epoch is 233 loss is 2.6250498560899196e-06\n",
      "epoch is 234 loss is 2.331968115088939e-06\n",
      "epoch is 235 loss is 2.5103324002383387e-06\n",
      "epoch is 236 loss is 2.1308439248947472e-06\n",
      "epoch is 237 loss is 2.448097402395849e-06\n",
      "epoch is 238 loss is 3.1425400161676086e-06\n",
      "epoch is 239 loss is 2.219041686583102e-06\n",
      "epoch is 240 loss is 2.2684659770445066e-06\n",
      "epoch is 241 loss is 2.1032403564128925e-06\n",
      "epoch is 242 loss is 2.7191300304951837e-06\n",
      "epoch is 243 loss is 1.952830741660004e-06\n",
      "epoch is 244 loss is 2.945149122327147e-06\n",
      "epoch is 245 loss is 3.1069372339560187e-06\n",
      "epoch is 246 loss is 1.8125297178529731e-06\n",
      "epoch is 247 loss is 2.64885229509523e-06\n",
      "epoch is 248 loss is 2.070981485139731e-06\n",
      "epoch is 249 loss is 1.9290134550612878e-06\n",
      "epoch is 250 loss is 3.197428948825581e-06\n",
      "epoch is 251 loss is 2.274138194419474e-06\n",
      "epoch is 252 loss is 2.34076778878082e-06\n",
      "epoch is 253 loss is 2.237113827921592e-06\n",
      "epoch is 254 loss is 1.9072398960254982e-06\n",
      "epoch is 255 loss is 2.0528356880927763e-06\n",
      "epoch is 256 loss is 2.0073883691945276e-06\n",
      "epoch is 257 loss is 2.4487198368017657e-06\n",
      "epoch is 258 loss is 2.0067890795665667e-06\n",
      "epoch is 259 loss is 1.679494407548499e-06\n",
      "epoch is 260 loss is 1.8381357027631732e-06\n",
      "epoch is 261 loss is 2.1879401997723925e-06\n",
      "epoch is 262 loss is 2.2324575061777973e-06\n",
      "epoch is 263 loss is 1.8282819955041163e-06\n",
      "epoch is 264 loss is 2.614027701455677e-06\n",
      "epoch is 265 loss is 2.199666887270208e-06\n",
      "epoch is 266 loss is 1.7738002071476758e-06\n",
      "epoch is 267 loss is 2.283899177002065e-06\n",
      "epoch is 268 loss is 1.948767741393879e-06\n",
      "epoch is 269 loss is 1.7193631071360942e-06\n",
      "epoch is 270 loss is 2.518665102561141e-06\n",
      "epoch is 271 loss is 1.8552446428706177e-06\n",
      "epoch is 272 loss is 3.6301949562486837e-06\n",
      "epoch is 273 loss is 1.5633157738282107e-06\n",
      "epoch is 274 loss is 1.7634304732354443e-06\n",
      "epoch is 275 loss is 2.2025666804882316e-06\n",
      "epoch is 276 loss is 2.8559226553191914e-06\n",
      "epoch is 277 loss is 1.5165413420958308e-06\n",
      "epoch is 278 loss is 1.7972685753013537e-06\n",
      "epoch is 279 loss is 1.9649114423653006e-06\n",
      "epoch is 280 loss is 2.0141034116592954e-06\n",
      "epoch is 281 loss is 2.2068404583548634e-06\n",
      "epoch is 282 loss is 2.2807044698209008e-06\n",
      "epoch is 283 loss is 2.43680435196427e-06\n",
      "epoch is 284 loss is 1.8990932253141932e-06\n",
      "epoch is 285 loss is 1.6817518240047566e-06\n",
      "epoch is 286 loss is 2.646189917807059e-06\n",
      "epoch is 287 loss is 1.5547938374718042e-06\n",
      "epoch is 288 loss is 2.3028927425195663e-06\n",
      "epoch is 289 loss is 1.7442293325395427e-06\n",
      "epoch is 290 loss is 1.8600038788157652e-06\n",
      "epoch is 291 loss is 2.1929604331192728e-06\n",
      "epoch is 292 loss is 2.27590942497401e-06\n",
      "epoch is 293 loss is 2.1470813694951236e-06\n",
      "epoch is 294 loss is 2.16750510761275e-06\n",
      "epoch is 295 loss is 1.5781281405909056e-06\n",
      "epoch is 296 loss is 1.8944831931263106e-06\n",
      "epoch is 297 loss is 1.6028281311740649e-06\n",
      "epoch is 298 loss is 1.9302664754930308e-06\n",
      "epoch is 299 loss is 1.6311833954632524e-06\n",
      "epoch is 300 loss is 2.0118797661998043e-06\n",
      "epoch is 301 loss is 1.6622020559930994e-06\n",
      "epoch is 302 loss is 1.596934218850498e-06\n",
      "epoch is 303 loss is 1.5471158849052728e-06\n",
      "epoch is 304 loss is 1.5703386855091888e-06\n",
      "epoch is 305 loss is 2.5681925983531804e-06\n",
      "epoch is 306 loss is 2.418970645508121e-06\n",
      "epoch is 307 loss is 1.451870319982289e-06\n",
      "epoch is 308 loss is 1.3693385163893287e-06\n",
      "epoch is 309 loss is 1.394599513099804e-06\n",
      "epoch is 310 loss is 2.0569535662286467e-06\n",
      "epoch is 311 loss is 4.298991700398546e-06\n",
      "epoch is 312 loss is 4.620134373984139e-06\n",
      "epoch is 313 loss is 2.1942856536130675e-06\n",
      "epoch is 314 loss is 3.4490048247190074e-06\n",
      "epoch is 315 loss is 2.1040856502845384e-06\n",
      "epoch is 316 loss is 1.4551749284675846e-06\n",
      "epoch is 317 loss is 4.418342187651324e-06\n",
      "epoch is 318 loss is 1.931622410126126e-06\n",
      "epoch is 319 loss is 2.1086128561915587e-06\n",
      "epoch is 320 loss is 2.6619947442936876e-06\n",
      "epoch is 321 loss is 2.172119215610587e-06\n",
      "epoch is 322 loss is 1.8774704712467218e-06\n",
      "epoch is 323 loss is 2.059649860078225e-06\n",
      "epoch is 324 loss is 3.862793278735144e-06\n",
      "epoch is 325 loss is 2.2473496423638883e-06\n",
      "epoch is 326 loss is 1.8686010720916897e-06\n",
      "epoch is 327 loss is 2.0575930316975878e-06\n",
      "epoch is 328 loss is 2.326804209441161e-06\n",
      "epoch is 329 loss is 5.699169541679721e-06\n",
      "epoch is 330 loss is 3.4192886765989226e-06\n",
      "epoch is 331 loss is 1.9502826326152016e-06\n",
      "epoch is 332 loss is 1.9011353516660655e-06\n",
      "epoch is 333 loss is 2.5846923504370605e-06\n",
      "epoch is 334 loss is 1.8961641736032815e-06\n",
      "epoch is 335 loss is 1.6814822382891913e-06\n",
      "epoch is 336 loss is 1.9659021553135394e-06\n",
      "epoch is 337 loss is 2.440409988127637e-06\n",
      "epoch is 338 loss is 1.4852551337553956e-06\n",
      "epoch is 339 loss is 1.8052214994751762e-06\n",
      "epoch is 340 loss is 5.403888755643617e-06\n",
      "epoch is 341 loss is 1.9146179854310874e-06\n",
      "epoch is 342 loss is 1.6879342454472495e-06\n",
      "epoch is 343 loss is 1.916038259635608e-06\n",
      "epoch is 344 loss is 2.2476337845435775e-06\n",
      "epoch is 345 loss is 1.649276934804004e-06\n",
      "epoch is 346 loss is 1.5578166328116707e-06\n",
      "epoch is 347 loss is 1.4154678056317591e-06\n",
      "epoch is 348 loss is 2.7100665936733865e-06\n",
      "epoch is 349 loss is 1.4599903523642482e-06\n",
      "epoch is 350 loss is 2.2641542068065126e-06\n",
      "epoch is 351 loss is 2.8099786698650517e-06\n",
      "epoch is 352 loss is 1.1613756265676405e-06\n",
      "epoch is 353 loss is 1.5228758785884133e-06\n",
      "epoch is 354 loss is 2.8825455548130523e-06\n",
      "epoch is 355 loss is 1.127201561309763e-06\n",
      "epoch is 356 loss is 1.7110967822896946e-06\n",
      "epoch is 357 loss is 1.643564541588056e-06\n",
      "epoch is 358 loss is 1.3554301060486973e-06\n",
      "epoch is 359 loss is 1.028707939764603e-06\n",
      "epoch is 360 loss is 1.839046209594092e-06\n",
      "epoch is 361 loss is 1.4728278436393208e-06\n",
      "epoch is 362 loss is 1.4008892156828862e-06\n",
      "epoch is 363 loss is 2.1137279976845233e-06\n",
      "epoch is 364 loss is 1.1530373200061508e-06\n",
      "epoch is 365 loss is 1.0195011945535109e-06\n",
      "epoch is 366 loss is 1.2256060972944875e-06\n",
      "epoch is 367 loss is 8.438189379188485e-07\n",
      "epoch is 368 loss is 1.233014173016171e-06\n",
      "epoch is 369 loss is 8.701665018938153e-07\n",
      "epoch is 370 loss is 1.0956230591697296e-06\n",
      "epoch is 371 loss is 1.0292321908200074e-06\n",
      "epoch is 372 loss is 7.479439514871685e-07\n",
      "epoch is 373 loss is 7.517575995228595e-07\n",
      "epoch is 374 loss is 8.607289639440467e-07\n",
      "epoch is 375 loss is 7.532673231994186e-07\n",
      "epoch is 376 loss is 4.169139306539295e-06\n",
      "epoch is 377 loss is 1.000085564358751e-06\n",
      "epoch is 378 loss is 6.653843450477325e-07\n",
      "epoch is 379 loss is 1.5237529055518484e-06\n",
      "epoch is 380 loss is 1.1903980136076038e-06\n",
      "epoch is 381 loss is 9.395222308561928e-07\n",
      "epoch is 382 loss is 7.805224827124798e-07\n",
      "epoch is 383 loss is 1.0626964828872278e-06\n",
      "epoch is 384 loss is 7.072830891219778e-07\n",
      "epoch is 385 loss is 8.507895918936236e-07\n",
      "epoch is 386 loss is 1.8463173089883745e-06\n",
      "epoch is 387 loss is 5.865218257575193e-07\n",
      "epoch is 388 loss is 5.473463413826411e-07\n",
      "epoch is 389 loss is 7.644324950936736e-07\n",
      "epoch is 390 loss is 7.199808383414655e-07\n",
      "epoch is 391 loss is 6.630631712786143e-07\n",
      "epoch is 392 loss is 9.032906821767924e-07\n",
      "epoch is 393 loss is 8.588302187637897e-07\n",
      "epoch is 394 loss is 7.484896733271551e-07\n",
      "epoch is 395 loss is 8.667617449353508e-07\n",
      "epoch is 396 loss is 8.036926616985897e-07\n",
      "epoch is 397 loss is 7.049812754501437e-07\n",
      "epoch is 398 loss is 1.0823499657057642e-06\n",
      "epoch is 399 loss is 5.578006119602694e-07\n",
      "epoch is 400 loss is 6.057118307583665e-07\n",
      "epoch is 401 loss is 5.113117780539907e-07\n",
      "epoch is 402 loss is 5.738810287039144e-07\n",
      "epoch is 403 loss is 2.924998900170399e-06\n",
      "epoch is 404 loss is 6.336262161404164e-07\n",
      "epoch is 405 loss is 1.1005749499175503e-06\n",
      "epoch is 406 loss is 7.327951120562611e-07\n",
      "epoch is 407 loss is 6.133032087545239e-07\n",
      "epoch is 408 loss is 6.1362476104701e-07\n",
      "epoch is 409 loss is 6.977148341243809e-07\n",
      "epoch is 410 loss is 5.15758413932088e-07\n",
      "epoch is 411 loss is 1.945610735454795e-06\n",
      "epoch is 412 loss is 4.819940403569635e-07\n",
      "epoch is 413 loss is 6.114230230660244e-07\n",
      "epoch is 414 loss is 5.063569032310247e-07\n",
      "epoch is 415 loss is 1.510284100427744e-06\n",
      "epoch is 416 loss is 1.127521002914951e-06\n",
      "epoch is 417 loss is 5.179747447683574e-07\n",
      "epoch is 418 loss is 9.097806817062127e-07\n",
      "epoch is 419 loss is 5.945821766365726e-07\n",
      "epoch is 420 loss is 7.457859556808737e-07\n",
      "epoch is 421 loss is 5.750222918823496e-07\n",
      "epoch is 422 loss is 8.077499849437065e-07\n",
      "epoch is 423 loss is 8.280508665041314e-07\n",
      "epoch is 424 loss is 7.145093546067599e-07\n",
      "epoch is 425 loss is 6.579019585766991e-07\n",
      "epoch is 426 loss is 7.4768579259594e-07\n",
      "epoch is 427 loss is 3.9346071577940407e-07\n",
      "epoch is 428 loss is 9.087448437191911e-07\n",
      "epoch is 429 loss is 4.737795365228434e-07\n",
      "epoch is 430 loss is 7.490700395517552e-07\n",
      "epoch is 431 loss is 5.202095623140635e-07\n",
      "epoch is 432 loss is 5.22944321620088e-07\n",
      "epoch is 433 loss is 4.7037394256769175e-07\n",
      "epoch is 434 loss is 3.7972665548781074e-07\n",
      "epoch is 435 loss is 5.26038443634159e-07\n",
      "epoch is 436 loss is 7.538436863963848e-07\n",
      "epoch is 437 loss is 1.9635231923818515e-06\n",
      "epoch is 438 loss is 4.1036662960348746e-07\n",
      "epoch is 439 loss is 4.939995569250627e-07\n",
      "epoch is 440 loss is 4.864205155322499e-07\n",
      "epoch is 441 loss is 5.139335428072572e-07\n",
      "epoch is 442 loss is 7.972924755528109e-07\n",
      "epoch is 443 loss is 1.2792092574764393e-06\n",
      "epoch is 444 loss is 5.40555568913271e-07\n",
      "epoch is 445 loss is 7.821744230435244e-07\n",
      "epoch is 446 loss is 4.774266586089626e-07\n",
      "epoch is 447 loss is 4.014296884549227e-07\n",
      "epoch is 448 loss is 6.969461072540238e-07\n",
      "epoch is 449 loss is 5.101656020689137e-07\n",
      "epoch is 450 loss is 5.302028660640638e-07\n",
      "epoch is 451 loss is 4.6974113668108703e-07\n",
      "epoch is 452 loss is 1.1707764094454976e-06\n",
      "epoch is 453 loss is 6.218747098712707e-07\n",
      "epoch is 454 loss is 7.639077345620248e-07\n",
      "epoch is 455 loss is 3.288169870360936e-07\n",
      "epoch is 456 loss is 4.307084512192594e-07\n",
      "epoch is 457 loss is 2.706103450751148e-07\n",
      "epoch is 458 loss is 3.887639270653461e-07\n",
      "epoch is 459 loss is 4.375745533904682e-07\n",
      "epoch is 460 loss is 4.3205667092636924e-07\n",
      "epoch is 461 loss is 1.5117023367272871e-06\n",
      "epoch is 462 loss is 5.117515288351588e-07\n",
      "epoch is 463 loss is 2.554933843006377e-07\n",
      "epoch is 464 loss is 9.990793487762205e-07\n",
      "epoch is 465 loss is 9.296238352522771e-07\n",
      "epoch is 466 loss is 4.186638141538285e-07\n",
      "epoch is 467 loss is 3.49714792799059e-07\n",
      "epoch is 468 loss is 2.3853372073277863e-07\n",
      "epoch is 469 loss is 4.642213982678123e-07\n",
      "epoch is 470 loss is 3.7872764535502146e-07\n",
      "epoch is 471 loss is 3.5846355525766845e-07\n",
      "epoch is 472 loss is 7.043533823701806e-07\n",
      "epoch is 473 loss is 5.32120170736078e-07\n",
      "epoch is 474 loss is 9.446903578785666e-07\n",
      "epoch is 475 loss is 3.4317799423748195e-07\n",
      "epoch is 476 loss is 5.146761044333525e-07\n",
      "epoch is 477 loss is 3.472502451365573e-06\n",
      "epoch is 478 loss is 3.209294577773454e-07\n",
      "epoch is 479 loss is 9.327167199668977e-07\n",
      "epoch is 480 loss is 6.978216057887261e-07\n",
      "epoch is 481 loss is 4.1920385897280687e-07\n",
      "epoch is 482 loss is 6.600122819629898e-07\n",
      "epoch is 483 loss is 1.3250851186784815e-06\n",
      "epoch is 484 loss is 2.6360778333098116e-06\n",
      "epoch is 485 loss is 7.645276943690406e-07\n",
      "epoch is 486 loss is 3.0537827767806713e-07\n",
      "epoch is 487 loss is 1.0288971737982072e-06\n",
      "epoch is 488 loss is 7.068521450016043e-07\n",
      "epoch is 489 loss is 5.136064954495646e-07\n",
      "epoch is 490 loss is 6.414661821526414e-07\n",
      "epoch is 491 loss is 7.808131025187035e-07\n",
      "epoch is 492 loss is 1.7306845426727138e-06\n",
      "epoch is 493 loss is 6.093569877218291e-07\n",
      "epoch is 494 loss is 4.092234013023955e-07\n",
      "epoch is 495 loss is 6.311650819687516e-07\n",
      "epoch is 496 loss is 9.257820204585073e-07\n",
      "epoch is 497 loss is 4.4106504792261957e-07\n",
      "epoch is 498 loss is 7.082981113653027e-07\n",
      "epoch is 499 loss is 3.9081391390784525e-07\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 6):\n",
    "optimizer = NoamOpt(hid_size, 1, 2000,\n",
    "          torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_iter = make_train_iter(64, 6)\n",
    "train(model, train_iter, optimizer, criterion, 1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'life', 'hands', 'you', 'lemons']\n",
      "when life hands you lemons\n",
      "\n",
      "when life hands you lemons irrational , leaving but the best , you love may be blind , love may be blind , love may\n"
     ]
    }
   ],
   "source": [
    "source_sentence = [\"when\",\"life\", \"hands\", \"you\", \"lemons\"]\n",
    "print(source_sentence)\n",
    "model.eval()\n",
    "print(' '.join(source_sentence))\n",
    "print()\n",
    "x = TEXT.numericalize([source_sentence]).to(device).squeeze(1)\n",
    "generated_sequence =model.generate_sequence(x)\n",
    "words = [TEXT.vocab.itos[word_idx] for word_idx in generated_sequence]\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'is', 'a']\n",
      "life is a\n",
      "\n",
      "life is a series of moments that old monkey english , anything , anything , anything , anything , anything , anything ,\n"
     ]
    }
   ],
   "source": [
    "source_sentence = [\"life\",\"is\", \"a\"]\n",
    "print(source_sentence)\n",
    "model.eval()\n",
    "print(' '.join(source_sentence))\n",
    "print()\n",
    "x = TEXT.numericalize([source_sentence]).to(device).squeeze(1)\n",
    "generated_sequence =model.generate_sequence(x)\n",
    "words = [TEXT.vocab.itos[word_idx] for word_idx in generated_sequence]\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'really', 'need', 'a']\n",
      "i really need a\n",
      "\n",
      "i really need a book , do now . to an i might i sit down i might i sit down i might i\n"
     ]
    }
   ],
   "source": [
    "source_sentence = [\"i\",\"really\", \"need\",\"a\"]\n",
    "print(source_sentence)\n",
    "model.eval()\n",
    "print(' '.join(source_sentence))\n",
    "print()\n",
    "x = TEXT.numericalize([source_sentence]).to(device).squeeze(1)\n",
    "generated_sequence =model.generate_sequence(x)\n",
    "words = [TEXT.vocab.itos[word_idx] for word_idx in generated_sequence]\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
