{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 1. Language Models - Introduction, Markov Chains and N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> “You shall know the nature of a word by the company it keeps.” – John\n",
    "> Rupert Firth\n",
    "\n",
    "Text Generation(NLG) is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. And a Language Model plays it part in each one of them. The goal of language modelling is to estimate the probability distribution of various linguistic units, e.g., words, sentences etc.([Source](https://medium.com/syncedreview/language-model-a-survey-of-the-state-of-the-art-technology-64d1a2e5a466)) In essence, we want our language model to learn the \"grammar\" and \"style\" of the text on which it was trained on.\n",
    "\n",
    "**Key concepts to know beforehand:**\n",
    "\n",
    "- [Probability](https://tinyheero.github.io/2016/03/20/basic-prob.html)\n",
    "\t- Joint Probability\n",
    "\t- Conditional Probability\n",
    " - [N-Grams](https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058)\n",
    " - [Markov Chains](https://setosa.io/ev/markov-chains/)\n",
    " - [Entropy, Cross Entropy](https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/)\n",
    "- Basic familiarity with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Toy Data  \n",
    "\n",
    "To illustrate and learn what Language Models are, we have taken the example - a tongue twister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data = \"\"\"To sit in solemn silence in a dull, dark dock,\n",
    "In a pestilential prison, with a life-long lock,\n",
    "Awaiting the sensation of a short, sharp shock,\n",
    "From a cheap and chippy chopper on a big black block,\n",
    "To sit in solemn silence in a dull, dark dock,\n",
    "In a pestilential prison, with a life-long lock,\n",
    "Awaiting the sensation of a short, sharp shock,\n",
    "From a cheap and chippy chopper on a big black block,\n",
    "A dull, dark dock, a life-long lock,\n",
    "A short, sharp shock, a big black block,\n",
    "To sit in solemn silence in a pestilential prison,\n",
    "And awaiting the sensation\n",
    "From a cheap and chippy chopper on a big black block!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Language Model   \n",
    "\n",
    "As discussed before, a Language Model learns the \"grammar\" and \"style\" of the text on which it was trained on. But how do we translate that to a mathematical statement? We know that there is no hard and fast rules in language. An idea can be expressed in a lot of different ways. Therefore stochasticity is built into language. So, it follows that the mathematical model we aim for language is also probabilistic.\n",
    "\n",
    "An LM (or what we casually refer to as an LM) is typically comprised of a few components as shown below.\n",
    "![LM Overview](images/lm_overview.png)\n",
    "\n",
    "\n",
    "1. The core Language Model which learns the \"style and grammar\" of the language.\n",
    "2. A mechanism to generate robust probability distribution over words - eg. Smoothing or Backoff in Statistical Language Models\n",
    "3. A mechanism to decode the probability distribution and generate text - eg. Greedy Decoding, Beam Search, top-k sampling, etc.\n",
    "4. A downstream task - eg. Machine Translation, Text Summarization, etc.\n",
    "\n",
    "There are two types of Language Models:\n",
    "1. Count-based Language Models or Statistical Language Models\n",
    "2. Continuous-space Language Models or Neural Language Models\n",
    "\n",
    "Let's start with Count-based Language models as it is a very good starting point to understand Language Models. And once you have understood the rest of the components, it is very easy to switch out the LM block with more recent and popular Neural LMs like BERT, GPT, etc.\n",
    "\n",
    "One of the ways of looking at a Language model is that it is a probabilistic model which knows the probability of a given word, provided the context. How can we formalize that? If the context can be defined as the preceeding words, then the Language Model becomes:   \n",
    "$ P(word | all\\;previous\\;words\\;in\\;context)$ or more formally,  \n",
    "$ P(w_n|w_1, w_2, w_3,...w_{n-1})$  \n",
    "\n",
    "Simple enough right? We can get away with simple counting, because after all this is Probability. But we have a couple of problems. \n",
    "1. What do we do with the first word in a sentence? It doesn't start with any other. So even with a context window on 2, we cannot estimate probabilities for that.\n",
    "2. What do we do when we are looking at a word somewhether in the middle of a large corpus? Our context will be half the corpus. This will get out of hand very very fast. It's just not computationally tractable to look at the entire history as the context.\n",
    "\n",
    "For problem 1, there is a very easy and straightforward solution. We add **<start>** and **<end>** tokens to the sentences. This let's us model the probability of the first word, given <start> token and also provide the model a token with which it can stop a sentence as well.\n",
    "\n",
    "For problem 2, we have the Markov Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Tokenization and Special Tokens \n",
    "\n",
    "To make things simple, we are just doing a split by word tokenization. But we also add **start** and **end** tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>', 'to', 'sit', 'in', 'solemn', 'silence', 'in', 'a', 'dull', 'dark']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "for sentence in data.split(\",\"):\n",
    "    tokens = tokens + [word.lower().strip() for word in sentence.lower().split()]\n",
    "\n",
    "tokens = ['<start>'] + tokens + ['end']\n",
    "\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Markov Assumption   \n",
    "So, we take the help of [Markov Property](https://en.wikipedia.org/wiki/Markov_property) to reduce the complexity of our Language Model. [Markov Property](https://en.wikipedia.org/wiki/Markov_property) states that the probability of any state depends only on the previous state and not on the entire sequence of events which preceded it. This helps us in dealing with the infinite context problem in the Language Model. Applying the Markov assumption to our Language Model, we can consider that the probability of the next word is only dependent on a fixed window(words in that window is the previous state) and not the entire history. This makes the language model simple enough to be feasible.\n",
    "\n",
    "$P(w_i|w_1,w_2,w_3...w_{i-1}) \\approx P(w_i|w_i-k ... w_{i-1})$\n",
    "\n",
    "If we consider the last two words as the state, the language model becomes:\n",
    "\n",
    "$P(w|w_{n-1}, w_{n-2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Language Model using Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Markov Chains  \n",
    "\n",
    "A Markov Chain is a stochastic process which statisfies Markov Property, i.e. a process where the past and future are independent of the present state(in our case, the words in the window of context). The most intuitive way of understanding the Language model is by using the Markov Chains directly. \n",
    "\n",
    "We can create a Markov Chain by considering multiple states and the transition probabilities from one state to the other. i.e. If my current state is $(w_{i-1}, w_{i-2}, w_{i-3})$, what is the probability that I move to $(w_{i}, w_{i-1}, w_{i-2})$. This can be seen as the probability that $w_i$ occurs after $(w_{i-1}, w_{i-2}, w_{i-3})$, and that can easily be estimated from the corpus.\n",
    "\n",
    "So, first we need to make context-target pairs out of our tokens. Let's define window as 3 i.e. we are looking back three tokens as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> to', 'sit'),\n",
       " ('to sit', 'in'),\n",
       " ('sit in', 'solemn'),\n",
       " ('in solemn', 'silence'),\n",
       " ('solemn silence', 'in'),\n",
       " ('silence in', 'a'),\n",
       " ('in a', 'dull'),\n",
       " ('a dull', 'dark'),\n",
       " ('dull dark', 'dock'),\n",
       " ('dark dock', 'in')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_target_pairs = []\n",
    "n=2\n",
    "for i in range(len(tokens)-n):\n",
    "        context, word = tokens[i:i+n], tokens[i+n]\n",
    "        context_target_pairs.append((\" \".join(context),word))\n",
    "context_target_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that we have the Context-Target pairs, we can calculate the probability of the coming word, given a context by simple counting. \n",
    "\n",
    "Now, let's see an example. In our short toy corpus, what is the probability that **'dull'** comes after **'in a'**?  \n",
    "The simplest way is to count the number of times **dull** occured after **'in a'** and then divide it by the number of times any word occured after **'in a'**. \n",
    "\n",
    "We initialize a *defaultdict* with a Counter inside to count the number of times a target comes after the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def count_markov_chain(context_target_pairs, order=4):\n",
    "    mc = defaultdict(Counter)\n",
    "    for context, target in context_target_pairs:\n",
    "        mc[context][target]+=1\n",
    "    return mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mc = count_markov_chain(context_target_pairs, order=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's see all the words which occured after **'in a'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'dull': 2, 'pestilential': 3})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc[\"in a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$P(dull| in\\;a) = \\frac{C_{dull| in\\;a}}{C_{in\\;a}}$   \n",
    "$P(dull | in\\;a) = \\frac {2}{5} = 0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Side Note: ConditionalFreqDist   \n",
    "\n",
    "The above can also be easily done with ConditionalFreqDist in nltk., which gives us richer objects instead of vanilla Counters and defaultdicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'pestilential': 3, 'dull': 2})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc = nltk.ConditionalFreqDist(context_target_pairs)\n",
    "mc[\"in a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Language Model from N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There is an alternate way of calculating the probabilities for the Language Model, although not as intuitive as the previous one.\n",
    "\n",
    "Let's start by taking a hard look at the formula from previous section\n",
    "$P(dull| in\\;a) = \\frac{C_{dull| in\\;a}}{C_{in\\;a}}$      \n",
    "\n",
    "What is this $C_{dull| in\\;a}$? Isn't it just the count of the trigram - **in a dull**?     \n",
    "\n",
    "And $C_{in\\;a}$ is just the count of the bigram - **in a**.\n",
    "\n",
    "If we generalize this to n-grams, \n",
    "\n",
    "$P(w_i|w_{i-n+1}^{i-1}) = \\frac{C(w_{i-n+1}^{i})}{C(w_{i-n+1}^{i-1})} = \\frac{C(n\\;gram)}{C(n-1\\;gram)}$   \n",
    "\n",
    "It's good to know both of these are the same because in the literature around, you will find both and may be confused as to which one is correct. Fear, not both are. NLTK uses the previous methodology in it's LMs(which is what we are going to use soon.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "NLTK has a lot of ready-to-use functions for the manipulation of corpus, sentences, words etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Count of Trigram **in a dull**: 2. Count of bigram **in a.**: 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$P(dull| in\\;a) = \\frac{2}{5} = 0.4$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigrams = list(nltk.ngrams(tokens, n=3))\n",
    "bigrams = list(nltk.ngrams(tokens, n=2))\n",
    "trigram_freq = nltk.FreqDist([\" \".join([w1,w2,w3]) for w1, w2, w3 in trigrams])\n",
    "bigram_freq = nltk.FreqDist([\" \".join([w1,w2]) for w1, w2 in bigrams])\n",
    "\n",
    "bigram_count = bigram_freq['in a']\n",
    "trigram_count = trigram_freq['in a dull']\n",
    "display (Markdown(f\"Count of Trigram **in a dull**: {trigram_count}. Count of bigram **in a.**: {bigram_count}\"))\n",
    "display (Latex(r\"$P(dull| in\\;a) = \\frac{\" + str(trigram_count)+\"}{\" +str(bigram_count)+\"} = \"+str(trigram_count/bigram_count)+\"$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This is exactly the probability we estimated through the nmarkov chain method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Evaluation  \n",
    "\n",
    "The evaluation of NLG models is a long debated and actively researched topic. [Hashimoto et al., 2019](https://nlp.stanford.edu/pubs/hashimoto2019unifying.pdf) proposes that _\"A good evaluation metric should not only capture the **quality of generation**, but also the **diversity of generation**, which is especially crucial for creative, open-ended tasks like dialogue or story generation.\"_ \n",
    "\n",
    "Human Evaluation, which is considered as the gold standard in NLG, only captures quality of the generated text while ignores diversity. for eg. If a Language Model regurgitates a sentence from the training corpus, it will pass the human evaluation with flying colors. But does the model have the generalization it shoyuld have?\n",
    "\n",
    "Statistical Evaluation metrics like Perplexity captures diversity and ensures the model assigns reasonable probability to unseen sentences.\n",
    "\n",
    "There are automated ways of imitating Human Evaluation - [BLEU, METEOR, and ROUGE](https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054) are metrics proposed by [Papineni et al., 2001](https://www.aclweb.org/anthology/P02-1040.pdf), [Banerjee et al., 2005](https://www.cs.cmu.edu/~alavie/METEOR/pdf/Banerjee-Lavie-2005-METEOR.pdf), and [Lin et al., 2004](https://www.aclweb.org/anthology/W04-1013.pdf), respectively. BLEU and METEOR are the de-facto standard in Machine Translation and ROUGE is often used in Text Summarization. Criticism of these metrics are also very prominent in the NLProc community. Multiple studies have shown that Human Evaluation and these automatic metrics do not enough correlation. Rachel Tatman has written an excellent [blog](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) on the subject warning us of using BLUE or any other automatic metric without really understanding it.\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "Perplexity is a close cousin of [Entropy](https://deep-and-shallow.com/2020/01/09/deep-learning-and-information-theory/). Wikipedia defines Perplexity as a measurement of how well a probability distribution or probability model predicts a sample. It can also seen as how well the language model anticipates unseen data.\n",
    "\n",
    "Mathematically, \n",
    "$Perplexity\\;for\\;a\\;Probability\\;Distribution = b^{H(p)}$, where $H(p)$ is the Entropy of the distribution and $b$ is the base of the log used, most commonly 2 or e.\n",
    "$Entropy = \\sum_{i=1}^{n} p(x_i)log_b(p(x_i))$\n",
    "\n",
    "So, we already know that Entropy is the expected number of bits needed to encode the information contained in a random variable. Perplexity is just the exponentiation of the same, which means it has the same information, but just expressed differently.\n",
    "\n",
    "Let's take a look at how we can apply this to Language Modelling(If you have not read my previous blogpost on Entropy, I urge you to. It will give you the intuition required for  further discussions).\n",
    "\n",
    "In Language Modelling, we are building a model with a distribution $q$ over the words which tries to mimic the real world distribution of $p$ over words as close as possible. In practice we do not know $p$, but only sampled $p'$ from the language. So, we use the definition of Perplexity with this sampled $p'$, $q$ and the Cross Entropy between these two.\n",
    "\n",
    "If we sample N sentences from the Language,\n",
    "$PP(p',q) = b^{-\\sum_{i=1}^{N} p'(x_i)log_b(q(x_i))}$, where $p(x_i)$ is the sampled probability of sentence i and $q(x_i)$ is the probability assigned to that sentence by our language model.\n",
    "\n",
    "For a reasonably large dataset we assume the probability $p(x_i) = \\frac{1}{N}$. \n",
    "(**Note:** This is more to give you an intuition about the assumptions made. For exact mathematical proof, check out The Shannon-McMillan-Breiman Theorem which states that when N is sufficiently large, the $p(x_i)$ is automatically inferred by the distribution of the sample we are working with.)\n",
    "\n",
    "And we already know $q(x) = P(w_1, w_2, ....w_k)$, for a sentence with k words.\n",
    "Using some bit of Probability magic, it becomes:\n",
    "\n",
    "$q(x) = \\prod_{j=1}^k P(w_j|w_1, w_2,...w_{j-1}$, which is nothing but the product of all the word probabilities estimated by our language model.\n",
    "\n",
    "Putting these back into the exponent,\n",
    "\n",
    "$-\\frac{1}{N}\\sum_{i=1}^{N} log_b(q(x_i))$   \n",
    "\n",
    "Taking the $\\frac{1}{N}$ inside the log term, it becomes,   \n",
    "\n",
    "$\\sum_{i=1}^{N} log_b(q(x_i^{-\\frac{1}{N}}))$.\n",
    "\n",
    "Now, using the log-exponent rule ($b^{log_b{x}} = x$), we rewrite the perplexity equation as,\n",
    "\n",
    "$PP(p',q) = \\sum_{i=1}^{N} log_b(q(x_i^{-\\frac{1}{N}}))$\n",
    "\n",
    "Simplifying it and removing Log,\n",
    "\n",
    "$PP(p',q) = (\\prod_{i=1}^{N} q(x_i))^{-\\frac{1}{N}}$    \n",
    "\n",
    "And this is the perplexity which is used in NLP as a measure of uncertainity of the language model. Lower the perplexity, better the model.\n",
    "\n",
    "Intuitively, if model is a good language model, the probability it assigns to a test set would be quite high; because the model is not surprised/perplexed by the test set data. And when the probability is high, Perplexity is low.\n",
    "\n",
    "## Perplexity and Meena\n",
    "\n",
    "[Adiwardhana et al., 2020](https://arxiv.org/pdf/2001.09977.pdf) introduced a large open-domain Conversational agent, Meena, introduced a new metric to evaluate the responses - Sensibleness and Specificity Average(SSA). This was a metric used to measure both the sensibleness and specificity of a human like chatbot and used a set of crowd workers to evaluate the output generated by the model. But for quick research iterations, they had chosen Perplexity as the measure. And what they found out at the end was that Perplexity had a large correlation(~0.95) with the SSA metric, which makes a great case to use Perplexity as the metric to optimize, epecially for open domain Convesational Chatbots \n",
    "\n",
    "## Sample Calculation\n",
    "Let's take an example from our corpus and calculate Perplexity. Here I have constructed a new sentence, from the markov chain as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_data = \"to sit in solemn silence in a pestilential prison and awaiting the sensation from a cheap and chippy chopper\"\n",
    "fake_data = \"Jack and Jill went to the prison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Not adding <end> token for illustration\n",
    "test_tokens = [\"<start>\"]+[word.strip().lower() for word in test_data.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "First, let's calculate the Probability of the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start>', 'to', 'sit') 1.0\n",
      "('to', 'sit', 'in') 1.0\n",
      "('sit', 'in', 'solemn') 1.0\n",
      "('in', 'solemn', 'silence') 1.0\n",
      "('solemn', 'silence', 'in') 1.0\n",
      "('silence', 'in', 'a') 1.0\n",
      "('in', 'a', 'pestilential') 0.6\n",
      "('a', 'pestilential', 'prison') 1.0\n",
      "('pestilential', 'prison', 'and') 0.3333333333333333\n",
      "('prison', 'and', 'awaiting') 1.0\n",
      "('and', 'awaiting', 'the') 1.0\n",
      "('awaiting', 'the', 'sensation') 1.0\n",
      "('the', 'sensation', 'from') 0.3333333333333333\n",
      "('sensation', 'from', 'a') 1.0\n",
      "('from', 'a', 'cheap') 1.0\n",
      "('a', 'cheap', 'and') 1.0\n",
      "('cheap', 'and', 'chippy') 1.0\n",
      "('and', 'chippy', 'chopper') 1.0\n",
      "Probability of the sentence: 0.06666666666666665\n"
     ]
    }
   ],
   "source": [
    "#Creating trigrams as the window. First two is the context and last one is the target\n",
    "trigrams = nltk.trigrams(test_tokens)\n",
    "#Initialize the probability as 1 and using the ConditionalFreqDist from nltk as the model\n",
    "prob = 1\n",
    "for trigram in trigrams:\n",
    "    _p = mc[\" \".join(trigram[:2])][trigram[-1]]/mc[\" \".join(trigram[:2])].N()\n",
    "    print(trigram, _p)\n",
    "    prob = prob * _p\n",
    "print(\"Probability of the sentence: {}\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, the perplexity is just the Nth root of the inverse of probability. In practice, we would add the log probabilities to avoid numeric underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 0.9767268341369304\n"
     ]
    }
   ],
   "source": [
    "perplexity = pow(1/prob, -(1/len(tokens)))\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Generalization and Out of Vocabulary Words\n",
    "\n",
    "The simplistic approach to Language Modelling we saw till now has some problems when it comes to generalizing out of sample data. There are two ways this problem can manifest itself\n",
    "1. A totally new word that the model hasn't seen shows up.\n",
    "2. A totally new context that the model hasn't seen shows up.\n",
    "\n",
    "Let's face it. No matter how huge the corpus is going to be, we are not going to cover every single possible combinations word tokens in the language. Neither all the words in the language. And this means that sooner or later, we are going to stumble across unseen data/OOV tokens. \n",
    "\n",
    "And let's see what happens when we lookup the probability of an out of vocabulary token.   \n",
    "\n",
    "Let's say the OOV token is **exotic**, given the context **in a**.\n",
    "\n",
    "In the naive way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'pestilential': 3, 'dull': 2})"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc['in a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$P(exotic| in\\;a) = \\frac{C_{exotic| in\\;a}}{C_{in\\;a}} = \\frac{0}{5} = 0$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And this zero probability may look harmless at fiorst glance, but let's think about what would happen to Perplexity if there is an OOv token. We multiple the probabilities of all the n-grams in the test corpus and even if we have a single OOV word, it would make the entire probability zero. And the inverse of the probability becomes infinity and by extension perplexity also infinity.\n",
    "\n",
    "\n",
    "Let's see how to deal with OOV in a later point of time. In the next part, let's try and formalize our intuition using an implementation of the LanguageModel from NLTK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
