{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 2. Language Models - NLTK Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, let's see how we cna implement what we have learned in the last section. The core logic required for training a statistical language model is pretty simple because it's all about counting number of occurences of different n-grams. The pseudo code looks something like this:\n",
    "```\n",
    "1. language_model = defaultdict(Counter)\n",
    "2. for i in range(len(tokens)-n):\n",
    "    1. context, word = tokens[i : i + n], tokens[i + n]\n",
    "    2. language_model[context][word] +=1\n",
    "```\n",
    "But, we'll be using the Language Model in NLTK as a starting point for two main reasons \n",
    "1. I don't believe in re-inventing the wheel. If there is a production grade solution, use it. Adapt it to your needs.\n",
    "2. The implementation is general enough so that we can make our modifications easily.\n",
    "\n",
    "And for the second reason, it's important to understand the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## NLTK LM - an exploration\n",
    "\n",
    "Let's take a toy corpus and explore the implementation and do it the NLTK way.\n",
    "\n",
    "The corpus is one of my favorite movie monologues of all time. It is from the movie Rocky Balboa where Rocky gives this rousing speech to his son the night before his fight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "I'd hold you up to say to your mother, \"this kid's gonna be the best kid in the world. This kid's gonna be somebody better than anybody I ever knew.\" And you grew up good and wonderful. It was great just watching you, every day was like a privilege. Then the time come for you to be your own man and take on the world, and you did.\n",
    "\n",
    "But somewhere along the line, you changed. You stopped being you. You let people stick a finger in your face and tell you you're no good. And when things got hard, you started looking for something to blame, like a big shadow. Let me tell you something you already know.\n",
    "\n",
    "The world ain't all sunshine and rainbows. It's a very mean and nasty place and I don't care how tough you are it will beat you to your knees and keep you there permanently if you let it. You, me, or nobody is gonna hit as hard as life. But it ain't about how hard you hit. It's about how hard you can get hit and keep moving forward. How much you can take and keep moving forward. That's how winning is done! Cause if you're willing to go through all the battling you got to go through to get where you want to get, who's got the right to stop you?\n",
    "\n",
    "I mean maybe some of you guys got something you never finished, something you really want to do, something you never said to someone, something... and you're told no, even after you paid your dues? Who's got the right to tell you that, who? Nobody! It's your right to listen to your gut, it ain't nobody's right to say no after you earned the right to be where you want to be and do what you want to do!\n",
    "\n",
    "Now if you know what you're worth then go out and get what you're worth. But ya gotta be willing to take the hits, and not pointing fingers saying you ain't where you wanna be because of him, or her, or anybody! Cowards do that and that ain't you! You're better than that! I'm always gonna love you no matter what. No matter what happens. You're my son and you're my blood. You're the best thing in my life. But until you start believing in yourself, ya ain't gonna have a life.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's get comfortable with a few nifty tools in the NTLK arsenal which will make out preprocessing simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Sentence Tokenizer\n",
    "\n",
    "We have to split the text into sentences first. We can either use a naive `split(\".\")` or use slightly sophisticated tokenizers like the Punkt Sentence Tokenizer\n",
    "\n",
    "> Punkt Sentence Tokenizer\n",
    "> \n",
    "> This tokenizer divides a text into a list of sentences by using an\n",
    "> unsupervised algorithm to build a model for abbreviation words,\n",
    "> collocations, and words that start sentences. It must be trained on a\n",
    "> large collection of plaintext in the target language before it can be\n",
    "> used.\n",
    "> \n",
    "> The NLTK data package includes a pre-trained Punkt tokenizer for\n",
    "> English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nI\\'d hold you up to say to your mother, \"this kid\\'s gonna be the best kid in the world.', 'This kid\\'s gonna be somebody better than anybody I ever knew.\"', 'And you grew up good and wonderful.', 'It was great just watching you, every day was like a privilege.', 'Then the time come for you to be your own man and take on the world, and you did.']\n"
     ]
    }
   ],
   "source": [
    "order = 3\n",
    "#using the punkt sentence tokenizer to split our text into sentences.\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Word Tokenizer\n",
    "\n",
    "Now we need to tokenize the sentences to each word tokens. We can do it the naive way and do a `sentence.split(\" )`. But to get more accurate results, we can use word_tokenize from NLTK. It uses the TreebankWordDetokenizer by default. The key actions it does are:\n",
    "1.  Standardize starting quotes\n",
    "2. Deals with punctuation\n",
    "3. Converts parentheses to tokens\n",
    "4. Uses contractions like 'gonna' from the list Robert MacIntyre compiled to split them as well\n",
    "\n",
    "And after tokenizing, we also need to pad both ends with special tokens and make words all lower case. We already talked abut why we put start and end tokens and we make everything lower to make it easier for the model to learn and consider \"What\" and \"what\" the same. But if we have a huge corpus to train with, then probably it makes sense to leave the capitalization in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'i', \"'d\", 'hold', 'you', 'up', 'to', 'say', 'to', 'your', 'mother', ',', '``', 'this', 'kid', \"'s\", 'gon', 'na', 'be', 'the', 'best', 'kid', 'in', 'the', 'world', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "#removing the '.' and putting in start and end tokens for each sentence\n",
    "tokens = []\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.replace(\".\",\"\").replace('\\n', ' ').replace('\\r', '')\n",
    "    sentence_tokens = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        sentence_tokens.append(word.lower())\n",
    "    tokens.append(nltk.lm.preprocessing.pad_both_ends(sentence_tokens,n=order))\n",
    "print (list(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### N-Gram Generator\n",
    "Now that you have word tokens, we can make any n-grams out of this using ready functions in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams_l = []\n",
    "for token in tokens:\n",
    "    n_grams_l.append(nltk.ngrams(token, n=order+1))\n",
    "\n",
    "list(n_grams_l[0])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Putting it all together - Preprocessing Pipeline\n",
    "Now that we have gone through the basic blcks, let's put all this into a pipeline. which we can reuse for another corpus as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def pad_tokens(sentence_tokens, order):\n",
    "    return nltk.lm.preprocessing.pad_both_ends(sentence_tokens,n=order)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    return sentence.replace(\".\",\"\").replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "def tokenize_to_lower_sentence(sentence):\n",
    "    sentence_tokens = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        sentence_tokens.append(word.lower())\n",
    "    return sentence_tokens\n",
    "\n",
    "def split_to_sentences(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def create_n_grams(tokens, order):\n",
    "    return nltk.ngrams(tokens, n=order)\n",
    "\n",
    "def lm_preprocessing_pipeline(text, order):\n",
    "    sentences = split_to_sentences(text)\n",
    "    padded_sentence = []\n",
    "    n_grams = []\n",
    "    for sentence in sentences:\n",
    "        sentence = clean_sentence(sentence)\n",
    "        sentence_tokens = tokenize_to_lower_sentence(sentence)\n",
    "        sentence_tokens = list(pad_tokens(sentence_tokens, order))\n",
    "        n_grams.append(create_n_grams(sentence_tokens, order))\n",
    "        padded_sentence += sentence_tokens\n",
    "    return n_grams, padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_grams, padded_sentence = lm_preprocessing_pipeline(text, order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<s>', 'i', \"'d\", 'hold', 'you', 'up', 'to', 'say', 'to']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sentence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'i'),\n",
       " ('<s>', 'i', \"'d\"),\n",
       " ('i', \"'d\", 'hold'),\n",
       " (\"'d\", 'hold', 'you'),\n",
       " ('hold', 'you', 'up')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(n_grams[0])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Basic Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's understand the Language Model implementation in NLTK so that we can confidently use it.\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "The general LanguageModel class is an abstract class with just three parameters:\n",
    "* `order` - The order of the Language model, or the length of the context This parameter is only used while generating new text from the model. We will be overwriting that function and we will talk about this parameter in depth then.\n",
    "* `vocabulary` - (Optional) Vocabulary is a way to maintain the vocabulary of the model. If not given, it will be built up during training.\n",
    "* `counter` - (Optional) Counter is the core engine of the model which counts the context - word pairs. If not given, this too will be build up during training.\n",
    "\n",
    "How the NGramCounter works is very important to understand.\n",
    "\n",
    "![Ngram Counter](images/ngram_counter.png)\n",
    "\n",
    "So, as you can see, Ngram Counter takes the window, splits it into context and word, and then counts the co-occurences. It is important to know this because this tells us that the order of the Language Model is actually one more than the context window you choose. For eg. A trigram model has a context window of 2.\n",
    "\n",
    "**Fit**\n",
    "The other key method in the class is the `fit` method. It does two primary actions:\n",
    "1. Updates the Vocabulary.\n",
    "2. Updates the Context-Word co-occurences\n",
    "\n",
    "**Other helper methods**\n",
    "* `context_counts` - A helper method which retrieves all the counts for a given context.\n",
    "* `entropy` and `perplexity` - Helper methods to quickly calculate Entropy and Perplexity, given a set of text ngrams\n",
    "* `generate` - A helper method to generate text from the Language Model. This is the method we override to modify our text generation process\n",
    "\n",
    "**How do we inherit?**\n",
    "There is a abstract method called `unmasked_score` in the class, which is what we should be defining for any new class that inherits the class. for eg. The MLE model just returns the count of the word, given context, as is. \n",
    "\n",
    "**Modified LanguageModel**\n",
    "\n",
    "The LanguageModel implementation in NLTK has a few problems/shortcomings. So, I have made some changes to the original LanguageModel to make it easy for our use case.\n",
    "The main things I've changed are:\n",
    "- By default, text generation from the LM model was not flexible. Since we want to look at different Sampling Strategies, I abstracted that part out\n",
    "- I have also included another method in the base class which calculates the probability score for the entire vocabulary, given a context. This will be useful when we generate text from the model.\n",
    "- In one of the models(InterpolatedModel, which we will cover in the future), I implemented a method to track recursion. (If you haven't followed this, ignore it. I swear it'll become clearer when we reach the part where we talk about Interpolated Smoothing.)\n",
    "\n",
    "The code for the new LanguageModel is in the `api.py` file. Now, let's import it and create MLE Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from lm.api import LanguageModel\n",
    "\n",
    "class MLE(LanguageModel):\n",
    "    \"\"\"Class for providing MLE ngram model scores.\n",
    "\n",
    "    Inherits initialization from BaseNgramModel.\n",
    "    \"\"\"\n",
    "\n",
    "    def unmasked_score(self, word, context=None):\n",
    "        \"\"\"Returns the MLE score for a word given a context.\n",
    "\n",
    "        Args:\n",
    "        - word is expcected to be a string\n",
    "        - context is expected to be something reasonably convertible to a tuple\n",
    "        \"\"\"\n",
    "        return self.context_counts(context).freq(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's initialize our trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "order = 3\n",
    "n_grams, padded_sentence = lm_preprocessing_pipeline(text, order=order)\n",
    "model = MLE(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, this is just the shell. It has not vocab or counts because we haven't fitted it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, let's fit this with the n-grams we have prepared and the padded sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 31/-%  Fitting the model... \\\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 177 items>\n"
     ]
    }
   ],
   "source": [
    "model.fit(n_grams, padded_sentence)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Working with the model takes some getting used to, especially how you query out data. The easiest way to query data is using the `context_counts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'worth': 2, 'my': 2, 'no': 1, 'willing': 1, 'told': 1, 'better': 1, 'the': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.context_counts(('you', \"'re\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['get', 'take'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.context_counts((\"you\",\"can\")).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The NGramCounter in the MLE model is saved under the attribute, `counts`, which has another attribute `_counts` which is where the magic happens. Let's see what that holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This is very similar to what we were doing earlier, a defaultdict with ConditionalFreqDist objects. This also has separate key value pairs for each n-gram. In our case, we passed only a fourgram, and therefore we have just one key for '4'. The NgramCounter also has easy getters, albeit an non-intuitive interface.\n",
    "\n",
    "Let's see how we can query out the count of \"you\" after the context \"about how hard\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['you','can']]['take']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can also look at all the contexts the model has seen by indexing the `model.counts` with the order. In this case `3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>'),\n",
       " ('<s>', 'i'),\n",
       " ('i', \"'d\"),\n",
       " (\"'d\", 'hold'),\n",
       " ('hold', 'you'),\n",
       " ('you', 'up'),\n",
       " ('up', 'to'),\n",
       " ('to', 'say'),\n",
       " ('say', 'to'),\n",
       " ('to', 'your')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.counts[3].keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are a couple of  other convenient functions like `score`, '`logscore`, and `perplexity` which we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(word='take',context=\"you can\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score for an unseen context-token pair\n",
    "model.score(word='hard',context=\"you can\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score for an OOV\n",
    "model.score(word='dragon',context=\"you can\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logscore(word='take',context=\"you can\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0%  Calculating Perplexity... /\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perplexity if an unseen context-token pair is present\n",
    "model.perplexity([['you','can', \"hard\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0%  Calculating Perplexity... \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perplexity if an unseen context-token pair is present\n",
    "model.perplexity([['you','can', \"dragon\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now how do we generate text from the model. There are many ways of generating text from a Language Model, but we have chosen the most simple and straightforward. We take each context, get the distribution of words after that context, and then choose the most probable one or the one with maximum likelihood.\n",
    "Now, let's try and generate some text from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "seed = ('but',\n",
    " 'it',\n",
    " 'ai',\n",
    " \"n't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0%  Generating words... /\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['all', 'sunshine', 'and', 'rainbows', '</s>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(num_words=5, text_seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's make the generated output more human-like. For that we need to use the same tokenizer we used to tokenize the sentences and convert them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0%  Generating words... \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'all sunshine and rainbows </s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize(model.generate(num_words=5, text_seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "All is well in language model paradise, isn't it? Let's try and generate a longer sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0%  Generating words... |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"nobody's right to say to your knees and keep moving forward </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize(model.generate(num_words=50, text_seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## OOV problem\n",
    "\n",
    "Looks like it is stuck at the end token, isn't it? If you look at the code for GreedySampler, you can see that it returns the stop token when it hasn't seen the context before. This is partly because of OOV/unseen data problem.\n",
    "\n",
    "We talked abut two kinds of problems and let's see how we can solve one of them (and save the rest for the next part in the series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### \\<UNK> token and Closed Vocabulary\n",
    "\n",
    "The problem that this technique solves is the first one - a brand new word that the model hasn't seen. By default, the LanguageModel is an open vocabulary problem. But by using this technique, we can convert it into a closed vocabulary problem. and the texhnique is simple - we can handle the OOV by using the special `<UNK>` token. What we do is replace the low frequency words with a special token `<UNK>` so that the rare occurences are bunched together under this special token and help us out of the OOV situation. The LanguageModel in NLTK already does this for you, using the vocabulary object. By default, the vocabulary in the model replaces all the words which has only occured once in the corpus with `<UNK>` and whenever we query for text, we pass the text through the vocabulary object so that it can replace the unknown tokens with `<UNK>`. We can also change the vocabulary object and instruct it to increase the UNK token cutoff so that more words are replace with <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 31/-%  Fitting the model... |\n"
     ]
    }
   ],
   "source": [
    "order = 3\n",
    "n_grams, padded_sentence = lm_preprocessing_pipeline(text, order=order)\n",
    "vocab = nltk.lm.Vocabulary(unk_cutoff=3)\n",
    "model = MLE(order, vocabulary=vocab)\n",
    "model.fit(n_grams, padded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0%  Generating words... /\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'place how hard you after know watching privilege shadow through line time all me really are worth up moving never this winning big all blame was is her not winning care blame beat stick mother happens knew cowards say always hits mother saying then great until guys paid listen earned'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize(model.generate(num_words=50, text_seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The ability to generate longer text sequences(although gibberish) is because the probability mass for the rare tokens are re-distributed among the <UNK> tokens and hence not getting stuck in a narrow context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
